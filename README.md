# Clickstream Guardian ğŸ›¡ï¸

![Architecture](diagram/architecture.png)

**ì‹¤ì‹œê°„ í´ë¦­ìŠ¤íŠ¸ë¦¼ ì´ìƒ íƒì§€ + ë°°ì¹˜ ë¶„ì„ ë°ì´í„° íŒŒì´í”„ë¼ì¸**

YOOCHOOSE ì „ììƒê±°ë˜ í´ë¦­ ë° êµ¬ë§¤ ë¡œê·¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‹¤ì‹œê°„ ì„¸ì…˜ ëª¨ë‹ˆí„°ë§, Raw ë°ì´í„° ë³´ì¡´, ì¼ë³„ ë°°ì¹˜ ë¶„ì„, API ì„œë¹™ì„ ìˆ˜í–‰í•˜ëŠ” ì—”ë“œíˆ¬ì—”ë“œ ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤.

## ğŸ“‹ ëª©ì°¨

- [í”„ë¡œì íŠ¸ ê°œìš”](#í”„ë¡œì íŠ¸-ê°œìš”)
- [ì£¼ìš” ê¸°ëŠ¥](#ì£¼ìš”-ê¸°ëŠ¥)
- [ì•„í‚¤í…ì²˜](#ì•„í‚¤í…ì²˜)
- [ê¸°ìˆ  ìŠ¤íƒ](#ê¸°ìˆ -ìŠ¤íƒ)
- [ì‹œì‘í•˜ê¸°](#ì‹œì‘í•˜ê¸°)
- [ì‚¬ìš©ë²•](#ì‚¬ìš©ë²•)
- [í”„ë¡œì íŠ¸ êµ¬ì¡°](#í”„ë¡œì íŠ¸-êµ¬ì¡°)
- [API ë¬¸ì„œ](#api-ë¬¸ì„œ)
- [ìš´ì˜ ë° ì¥ì•  ëŒ€ì‘ ë¬¸ì„œ](#ìš´ì˜-ë°-ì¥ì• -ëŒ€ì‘-ë¬¸ì„œ)

## ğŸ¯ í”„ë¡œì íŠ¸ ê°œìš”

ë³¸ í”„ë¡œì íŠ¸ëŠ” ì‹¤ì œ ì„œë¹„ìŠ¤ í™˜ê²½ì—ì„œ ë°œìƒí•  ìˆ˜ ìˆëŠ” **íŠ¹ì • ì„¸ì…˜ í­ì£¼ì™€ ê°™ì€ ê³¼ë¶€í•˜ ìƒí™©**ì„ ê°€ì •í•˜ì—¬ ë‹¤ìŒ ì„¤ê³„ ì›ì¹™ì„ ë”°ë¦…ë‹ˆë‹¤:

- âœ… KafkaëŠ” **ìˆ˜ì§‘ ë° ì™„ì¶© ê³„ì¸µ**ìœ¼ë¡œ ì‚¬ìš©
- âœ… Raw ë°ì´í„°ëŠ” **Kafka ìœ ì… ì¦‰ì‹œ S3ì— ì˜êµ¬ ì €ì¥**
- âœ… Spark Streamingì€ **ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì „ìš©**
- âœ… RDBëŠ” **ìƒíƒœ/ì§‘ê³„ ì„œë¹™ ì „ìš©** (ê³ ë¹ˆë„ ì´ë²¤íŠ¸ ì €ì¥ íšŒí”¼)

## âœ¨ ì£¼ìš” ê¸°ëŠ¥

### 1. ì‹¤ì‹œê°„ ì´ìƒ íƒì§€ (Spark Structured Streaming)
- 10ì´ˆ Tumbling Window: ê³ ë¹ˆë„ í´ë¦­ íƒì§€ (50+ clicks)
- 1ë¶„ Sliding Window: ë´‡ íŒ¨í„´ íƒì§€ (100+ clicks + ë‚®ì€ ì•„ì´í…œ ë‹¤ì–‘ì„±)
- ì‹¤ì‹œê°„ìœ¼ë¡œ PostgreSQLì— ì´ìƒ ì„¸ì…˜ ì €ì¥

### 2. ë°ì´í„° ë ˆì´í¬ (S3/MinIO)
- Kafka Connectë¥¼ í†µí•œ ìë™ Raw ë°ì´í„° ì ì¬
- Parquet í¬ë§· + Snappy ì••ì¶•
- ì‹œê°„ ê¸°ë°˜ íŒŒí‹°ì…”ë‹ (`dt=YYYY-MM-DD/hour=HH`)

### 3. ì¼ë³„ ë°°ì¹˜ ë¶„ì„ (Spark Batch + Airflow)
- **Daily Metrics**: í´ë¦­/êµ¬ë§¤/ì „í™˜ìœ¨/ë§¤ì¶œ í†µê³„
- **Session Funnel**: ë·° â†’ ë©€í‹°ë·° â†’ êµ¬ë§¤ ì „í™˜ í¼ë„
- **Popular Items**: Top 100 ìƒí’ˆ (í´ë¦­/êµ¬ë§¤/ë§¤ì¶œ ê¸°ì¤€)

### 4. API ì„œë¹™ (FastAPI)
- RESTful APIë¡œ ì‹¤ì‹œê°„/ë°°ì¹˜ ë°ì´í„° ì œê³µ
- Swagger UI ìë™ ìƒì„±
- PostgreSQL Read-only ì¿¼ë¦¬

### 5. ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ (Streamlit)
- ì‹¤ì‹œê°„ ì´ìƒ íƒì§€ í˜„í™©
- ì¼ë³„ ë©”íŠ¸ë¦­ ì‹œê°í™”
- ì¸ê¸° ìƒí’ˆ/ì¹´í…Œê³ ë¦¬ ë¶„ì„

## ğŸ—ï¸ ì•„í‚¤í…ì²˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   CSV Data  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Kafka Producers â”‚â”€â”€â”€â”€â”€â–¶â”‚    Kafka     â”‚
â”‚  (Avro + SR)    â”‚      â”‚  (3 Brokers) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚            â”‚            â”‚
                    â–¼            â–¼            â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚   Kafka     â”‚ â”‚  Spark   â”‚ â”‚    Spark     â”‚
          â”‚  Connect    â”‚ â”‚Streaming â”‚ â”‚    Batch     â”‚
          â”‚ (S3 Sink)   â”‚ â”‚          â”‚ â”‚  (Airflow)   â”‚
          â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚             â”‚               â”‚
                 â–¼             â–¼               â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚   S3     â”‚   â”‚PostgreSQLâ”‚   â”‚PostgreSQLâ”‚
          â”‚(MinIO)   â”‚   â”‚(Anomaly) â”‚   â”‚(Metrics) â”‚
          â”‚Data Lake â”‚   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚               â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â–¼
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚   FastAPI     â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â–¼
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚   Dashboard   â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ› ï¸ ê¸°ìˆ  ìŠ¤íƒ

| Category | Technologies |
|----------|-------------|
| **Data Ingestion** | Apache Kafka 3.5, Schema Registry, Kafka Connect |
| **Streaming** | Spark Structured Streaming 3.5.0 |
| **Batch Processing** | Apache Spark 3.5.0, Apache Airflow 2.8.0 |
| **Storage** | MinIO (S3), PostgreSQL 15 |
| **API** | FastAPI 0.109.0, Uvicorn |
| **Dashboard** | Streamlit 1.30.0 |
| **Serialization** | Apache Avro |
| **Container** | Docker, Docker Compose |
| **Language** | Python 3.11 |

## ğŸš€ ì‹œì‘í•˜ê¸°

### ì‚¬ì „ ìš”êµ¬ì‚¬í•­

- Docker Desktop (Windows/Mac) ë˜ëŠ” Docker + Docker Compose (Linux)
- Python 3.11+
- ìµœì†Œ 16GB RAM ê¶Œì¥
- ìµœì†Œ 50GB ë””ìŠ¤í¬ ê³µê°„

### 1. í”„ë¡œì íŠ¸ í´ë¡ 

```bash
git clone https://github.com/yourusername/Clickstream-Guardian.git
cd Clickstream-Guardian
```

### 2. ì „ì²´ ì¸í”„ë¼ ì‹œì‘

```bash
cd docker
docker-compose up -d
```

### 3. ì´ˆê¸° ì„¤ì • (Kafka í† í”½, ìŠ¤í‚¤ë§ˆ, Connector)

```bash
# Kafka í† í”½ ìƒì„±
bash scripts/create_topics.sh

# Avro ìŠ¤í‚¤ë§ˆ ë“±ë¡
cd producers
python common/schema_registry.py http://localhost:8081 ../schemas

# Kafka Connect ì„¤ì •
bash ../scripts/setup_connectors.sh
```

### 4. ë°ì´í„° í”„ë¡œë“€ì‹±

```bash
cd producers

# í´ë¦­ ì´ë²¤íŠ¸ ì „ì†¡ (ìƒ˜í”Œ 10,000ê±´)
python producer_clicks.py --max-events 10000

# êµ¬ë§¤ ì´ë²¤íŠ¸ ì „ì†¡ (ìƒ˜í”Œ 1,000ê±´)
python producer_purchases.py --max-events 1000
```

### 5. Spark Streaming ì‹œì‘

```bash
docker exec spark-master spark-submit \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,\
              org.apache.spark:spark-avro_2.12:3.5.0,\
              org.postgresql:postgresql:42.6.0 \
  --master spark://spark-master:7077 \
  /opt/spark-streaming/anomaly_detector.py
```

### 6. ë°°ì¹˜ ë¶„ì„ ì‹¤í–‰ (íŠ¹ì • ë‚ ì§œ)

```bash
# ì¼ë³„ ë©”íŠ¸ë¦­
docker exec spark-master spark-submit \
  --packages org.postgresql:postgresql:42.6.0,org.apache.hadoop:hadoop-aws:3.3.4 \
  /opt/spark-batch/daily_metrics.py 2014-04-07

# ì„¸ì…˜ í¼ë„
docker exec spark-master spark-submit \
  --packages org.postgresql:postgresql:42.6.0,org.apache.hadoop:hadoop-aws:3.3.4 \
  /opt/spark-batch/session_funnel.py 2014-04-07

# ì¸ê¸° ìƒí’ˆ
docker exec spark-master spark-submit \
  --packages org.postgresql:postgresql:42.6.0,org.apache.hadoop:hadoop-aws:3.3.4 \
  /opt/spark-batch/popular_items.py 2014-04-07
```

## ğŸ“Š ì‚¬ìš©ë²•

### ì„œë¹„ìŠ¤ ì ‘ì†

| Service | URL | Credentials |
|---------|-----|-------------|
| **Dashboard** | http://localhost:8501 | - |
| **API (Swagger)** | http://localhost:8000/docs | - |
| **Airflow** | http://localhost:8082 | admin / admin |
| **MinIO Console** | http://localhost:9001 | minioadmin / minioadmin |
| **Spark Master** | http://localhost:8080 | - |

### API ì˜ˆì œ

```bash
# ì´ìƒ ì„¸ì…˜ ì¡°íšŒ
curl http://localhost:8000/anomalies?limit=10

# ì¼ë³„ ë©”íŠ¸ë¦­ ì¡°íšŒ
curl "http://localhost:8000/metrics/daily?start_date=2014-04-07&end_date=2014-04-10"

# ì¸ê¸° ìƒí’ˆ ì¡°íšŒ
curl http://localhost:8000/items/popular/2014-04-07?limit=50
```

### ë¶€í•˜ í…ŒìŠ¤íŠ¸ (ì„ íƒ)

```bash
# Locust ì„¤ì¹˜
pip install locust

# ë¶€í•˜ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
locust -f scripts/load_test.py --host http://localhost:8000
```

ì›¹ UI: http://localhost:8089

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
Clickstream-Guardian/
â”œâ”€â”€ data/                       # ì›ë³¸ CSV ë°ì´í„°
â”‚   â”œâ”€â”€ yoochoose-clicks.dat
â”‚   â””â”€â”€ yoochoose-buys.dat
â”‚
â”œâ”€â”€ docker/                     # Docker ì„¤ì •
â”‚   â”œâ”€â”€ docker-compose.yml
â”‚   â”œâ”€â”€ spark/Dockerfile
â”‚   â”œâ”€â”€ airflow/Dockerfile
â”‚   â””â”€â”€ postgres/init.sql
â”‚
â”œâ”€â”€ schemas/                    # Avro ìŠ¤í‚¤ë§ˆ
â”‚   â”œâ”€â”€ click-event.avsc
â”‚   â”œâ”€â”€ purchase-event.avsc
â”‚   â””â”€â”€ anomaly-event.avsc
â”‚
â”œâ”€â”€ producers/                  # Kafka Producer
â”‚   â”œâ”€â”€ common/
â”‚   â”‚   â”œâ”€â”€ config.py
â”‚   â”‚   â””â”€â”€ schema_registry.py
â”‚   â”œâ”€â”€ producer_clicks.py
â”‚   â””â”€â”€ producer_purchases.py
â”‚
â”œâ”€â”€ connectors/                 # Kafka Connect ì„¤ì •
â”‚   â”œâ”€â”€ s3-sink-clicks.json
â”‚   â””â”€â”€ s3-sink-purchases.json
â”‚
â”œâ”€â”€ spark-streaming/            # Spark Streaming
â”‚   â”œâ”€â”€ common/
â”‚   â”‚   â”œâ”€â”€ kafka_utils.py
â”‚   â”‚   â””â”€â”€ postgres_utils.py
â”‚   â””â”€â”€ anomaly_detector.py
â”‚
â”œâ”€â”€ spark-batch/                # Spark Batch
â”‚   â”œâ”€â”€ common/s3_utils.py
â”‚   â”œâ”€â”€ daily_metrics.py
â”‚   â”œâ”€â”€ session_funnel.py
â”‚   â””â”€â”€ popular_items.py
â”‚
â”œâ”€â”€ airflow/                    # Airflow DAG
â”‚   â””â”€â”€ dags/
â”‚       â””â”€â”€ daily_batch_pipeline.py
â”‚
â”œâ”€â”€ api/                        # FastAPI
â”‚   â”œâ”€â”€ models/database.py
â”‚   â”œâ”€â”€ routers/
â”‚   â”‚   â”œâ”€â”€ anomaly.py
â”‚   â”‚   â”œâ”€â”€ metrics.py
â”‚   â”‚   â””â”€â”€ sessions.py
â”‚   â”œâ”€â”€ config.py
â”‚   â””â”€â”€ main.py
â”‚
â”œâ”€â”€ dashboard/                  # Streamlit Dashboard
â”‚   â””â”€â”€ app.py
â”‚
â”œâ”€â”€ scripts/                    # ìœ í‹¸ë¦¬í‹° ìŠ¤í¬ë¦½íŠ¸
â”‚   â”œâ”€â”€ setup.sh
â”‚   â”œâ”€â”€ start.sh
â”‚   â”œâ”€â”€ stop.sh
â”‚   â”œâ”€â”€ create_topics.sh
â”‚   â”œâ”€â”€ setup_connectors.sh
â”‚   â””â”€â”€ load_test.py
â”‚
â””â”€â”€ README.md
```

## ğŸ“– API ë¬¸ì„œ

FastAPIëŠ” ìë™ìœ¼ë¡œ Swagger UIë¥¼ ìƒì„±í•©ë‹ˆë‹¤: http://localhost:8000/docs

### ì£¼ìš” ì—”ë“œí¬ì¸íŠ¸

#### Anomalies
- `GET /anomalies` - ìµœê·¼ ì´ìƒ ì„¸ì…˜ ì¡°íšŒ
- `GET /anomalies/types` - ì´ìƒ ìœ í˜•ë³„ í†µê³„
- `GET /anomalies/timeline` - ì‹œê°„ëŒ€ë³„ ì´ìƒ ë°œìƒ ì¶”ì´
- `GET /anomalies/{session_id}` - íŠ¹ì • ì„¸ì…˜ ì´ìƒ ì¡°íšŒ

#### Metrics
- `GET /metrics/daily` - ì¼ë³„ ë©”íŠ¸ë¦­ ì¡°íšŒ
- `GET /metrics/daily/{date}` - íŠ¹ì • ë‚ ì§œ ë©”íŠ¸ë¦­
- `GET /metrics/funnel/{date}` - ì „í™˜ í¼ë„ ë¶„ì„
- `GET /metrics/summary` - ìš”ì•½ í†µê³„

#### Items
- `GET /items/popular/{date}` - ì¸ê¸° ìƒí’ˆ
- `GET /items/categories/{date}` - ì¸ê¸° ì¹´í…Œê³ ë¦¬
- `GET /items/trending` - íŠ¸ë Œë”© ìƒí’ˆ

## ğŸ”§ ì„¤ì •

### í™˜ê²½ ë³€ìˆ˜

Producer, API ë“±ì—ì„œ ì‚¬ìš©í•˜ëŠ” ì£¼ìš” í™˜ê²½ ë³€ìˆ˜:

```bash
# Kafka
KAFKA_BOOTSTRAP_SERVERS=localhost:9092
SCHEMA_REGISTRY_URL=http://localhost:8081

# Database
DATABASE_URL=postgresql://admin:password@localhost:5432/clickstream

# Replay Speed (1.0 = ì‹¤ì‹œê°„, 100.0 = 100ë°°ì†)
REPLAY_SPEED=1.0
```

### Spark ì„¤ì •

`spark-batch/daily_metrics.py` ë“±ì—ì„œ Spark ì„¤ì • ë³€ê²½ ê°€ëŠ¥:

```python
spark = SparkSession.builder \
    .appName("DailyMetrics") \
    .config("spark.executor.memory", "4g") \
    .config("spark.executor.cores", "2") \
    .getOrCreate()
```

## ğŸ§ª í…ŒìŠ¤íŠ¸

```bash
# API í—¬ìŠ¤ ì²´í¬
curl http://localhost:8000/health

# Kafka í† í”½ í™•ì¸
docker exec kafka-1 kafka-topics --list --bootstrap-server kafka-1:29092

# PostgreSQL ë°ì´í„° í™•ì¸
docker exec -it postgres psql -U admin -d clickstream -c "SELECT COUNT(*) FROM anomaly_sessions;"
```

## ğŸ›‘ ì¤‘ì§€ ë° ì •ë¦¬

```bash
# ëª¨ë“  ì„œë¹„ìŠ¤ ì¤‘ì§€
bash scripts/stop.sh

# ë³¼ë¥¨ê¹Œì§€ ì‚­ì œ (ë°ì´í„° ì´ˆê¸°í™”)
cd docker
docker-compose down -v
```

## ğŸ“š ìš´ì˜ ë° ì¥ì•  ëŒ€ì‘ ë¬¸ì„œ

ì‹œìŠ¤í…œ ìš´ì˜, ë¶€í•˜ í…ŒìŠ¤íŠ¸, ì¥ì•  ëŒ€ì‘ì— ëŒ€í•œ ìƒì„¸ ë¬¸ì„œê°€ ì¤€ë¹„ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

### ğŸ“– ì£¼ìš” ë¬¸ì„œ
- **[ìš´ì˜ ë¬¸ì„œ ì¸ë±ìŠ¤](docs/OPERATIONS_README.md)** - ëª¨ë“  ìš´ì˜ ë¬¸ì„œì˜ ì‹œì‘ì 
- **[ë¶€í•˜ ì‹œë‚˜ë¦¬ì˜¤ ì„¤ê³„](docs/LOAD_TEST_SCENARIO.md)** - ë¶€í•˜ í…ŒìŠ¤íŠ¸ ê³„íš ë° ì‹¤í–‰
- **[ì¥ì•  ì‹œë‚˜ë¦¬ì˜¤ ë° ëŒ€ì‘ ì „ëµ](docs/FAILURE_RECOVERY_STRATEGY.md)** - ì»´í¬ë„ŒíŠ¸ë³„ ì¥ì•  ë³µêµ¬ Runbook
- **[ëª¨ë‹ˆí„°ë§ ì „ëµ](docs/MONITORING_STRATEGY.md)** - ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ ì§€í‘œ ë° ìˆ˜ì§‘ ë°©ë²•
- **[Fallback / Alert ì „ëµ](docs/FALLBACK_ALERT_STRATEGY.md)** - ì•Œë¦¼ ì„¤ì • ë° Fallback ë¡œì§

### ğŸ› ï¸ ì¥ì•  ì‹œë®¬ë ˆì´ì…˜
ì‹¤ì œ ì¥ì•  ìƒí™©ì„ ì‹œë®¬ë ˆì´ì…˜í•˜ì—¬ ë³µêµ¬ ì ˆì°¨ë¥¼ í…ŒìŠ¤íŠ¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```bash
# ì‹œë‚˜ë¦¬ì˜¤ ëª©ë¡ í™•ì¸
python scripts/failure_simulation.py --list

# Kafka Broker Down ì‹œë‚˜ë¦¬ì˜¤ ì‹¤í–‰
python scripts/failure_simulation.py --scenario 1

# ëŒ€í™”í˜• ëª¨ë“œ
python scripts/failure_simulation.py
```

**ì‚¬ìš© ê°€ëŠ¥í•œ ì‹œë‚˜ë¦¬ì˜¤**:
1. Kafka Broker Down - Broker ê°•ì œ ì¢…ë£Œ ë° Failover í…ŒìŠ¤íŠ¸
2. Consumer Lag Spike - Spark Streaming ì¤‘ì§€ë¡œ Lag ë°œìƒ
3. Spark OOM (Simulated) - Worker ê°•ì œ ì¢…ë£Œë¡œ OOM ì‹œë®¬ë ˆì´ì…˜
4. PostgreSQL Connection Pool Exhaustion - ë‹¤ìˆ˜ ì—°ê²° ìƒì„±
5. Data Corruption (DLQ Test) - ì˜ëª»ëœ ìŠ¤í‚¤ë§ˆ ì „ì†¡

### ğŸ“Š ì‹œìŠ¤í…œ ëŒ€ì‹œë³´ë“œ
- **Streamlit Dashboard**: http://localhost:8501 - ë¹„ì¦ˆë‹ˆìŠ¤ ë©”íŠ¸ë¦­
- **Spark Master UI**: http://localhost:8080 - Spark í´ëŸ¬ìŠ¤í„° ìƒíƒœ
- **Spark Streaming UI**: http://localhost:4040 - ì‹¤ì‹œê°„ Job ëª¨ë‹ˆí„°ë§
- **Airflow UI**: http://localhost:8082 - DAG ì‹¤í–‰ ìƒíƒœ
- **Kafka Connect**: http://localhost:8083 - Connector ìƒíƒœ

---

## ğŸ“ ë¼ì´ì„¼ìŠ¤

MIT License

## ğŸ‘¥ ê¸°ì—¬

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“§ ë¬¸ì˜

í”„ë¡œì íŠ¸ì— ëŒ€í•œ ì§ˆë¬¸ì´ë‚˜ ì œì•ˆì‚¬í•­ì´ ìˆìœ¼ì‹œë©´ ì´ìŠˆë¥¼ ë“±ë¡í•´ì£¼ì„¸ìš”.

---

**Built with â¤ï¸ using Kafka, Spark, Airflow, FastAPI, and Streamlit**
