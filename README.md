# ğŸ›¡ï¸ Clickstream Guardian

**ì‹¤ì‹œê°„ í´ë¦­ìŠ¤íŠ¸ë¦¼ ì´ìƒ íƒì§€ ë° ë°°ì¹˜ ë¶„ì„ ë°ì´í„° íŒŒì´í”„ë¼ì¸**

![Architecture](diagram/architecture.png)

YOOCHOOSE ì „ììƒê±°ë˜ í´ë¦­ ë° êµ¬ë§¤ ë¡œê·¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‹¤ì‹œê°„ ì„¸ì…˜ ëª¨ë‹ˆí„°ë§, Raw ë°ì´í„° ë³´ì¡´, ì¼ë³„ ë°°ì¹˜ ë¶„ì„, API ì„œë¹™ì„ ìˆ˜í–‰í•˜ëŠ” ì—”ë“œíˆ¬ì—”ë“œ ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤.

---

## ğŸ“‹ ëª©ì°¨

- [í”„ë¡œì íŠ¸ ê°œìš”](#-í”„ë¡œì íŠ¸-ê°œìš”)
- [ì£¼ìš” ê¸°ëŠ¥](#-ì£¼ìš”-ê¸°ëŠ¥)
- [ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜](#-ì‹œìŠ¤í…œ-ì•„í‚¤í…ì²˜)
- [ê¸°ìˆ  ìŠ¤íƒ](#-ê¸°ìˆ -ìŠ¤íƒ)
- [ë¹ ë¥¸ ì‹œì‘](#-ë¹ ë¥¸-ì‹œì‘)
- [ìƒì„¸ ì‚¬ìš© ê°€ì´ë“œ](#-ìƒì„¸-ì‚¬ìš©-ê°€ì´ë“œ)
- [í”„ë¡œì íŠ¸ êµ¬ì¡°](#-í”„ë¡œì íŠ¸-êµ¬ì¡°)
- [API ë¬¸ì„œ](#-api-ë¬¸ì„œ)
- [ëª¨ë‹ˆí„°ë§ ë° ëŒ€ì‹œë³´ë“œ](#-ëª¨ë‹ˆí„°ë§-ë°-ëŒ€ì‹œë³´ë“œ)
- [ìš´ì˜ ë° ì¥ì•  ëŒ€ì‘](#-ìš´ì˜-ë°-ì¥ì• -ëŒ€ì‘)
- [í…ŒìŠ¤íŠ¸](#-í…ŒìŠ¤íŠ¸)

---

## ğŸ¯ í”„ë¡œì íŠ¸ ê°œìš”

### ì„¤ê³„ ì² í•™

ë³¸ í”„ë¡œì íŠ¸ëŠ” **ì‹¤ì‹œê°„ ë°ì´í„° ì²˜ë¦¬ì™€ ë°°ì¹˜ ë¶„ì„ì„ ê²°í•©í•œ Lambda ì•„í‚¤í…ì²˜**ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤. ì‹¤ì œ í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ ë°œìƒí•  ìˆ˜ ìˆëŠ” ëŒ€ìš©ëŸ‰ íŠ¸ë˜í”½ê³¼ ì¥ì•  ìƒí™©ì„ ê³ ë ¤í•œ ì„¤ê³„ ì›ì¹™ì„ ë”°ë¦…ë‹ˆë‹¤:

#### í•µì‹¬ ì„¤ê³„ ì›ì¹™

| ì›ì¹™ | ì„¤ëª… | êµ¬í˜„ |
|------|------|------|
| **ğŸ”„ ë°ì´í„° ì˜ì†ì„±** | Raw ë°ì´í„°ëŠ” ìœ ì‹¤ë˜ì§€ ì•Šë„ë¡ S3ì— ì¦‰ì‹œ ì €ì¥ | Kafka Connect S3 Sink |
| **âš¡ ì‹¤ì‹œê°„ ì²˜ë¦¬** | ìŠ¤íŠ¸ë¦¬ë°ì€ ê°€ë²¼ìš´ ëª¨ë‹ˆí„°ë§ ìœ„ì£¼ | Spark Structured Streaming |
| **ğŸ“Š ë°°ì¹˜ ë¶„ì„** | ë³µì¡í•œ ì§‘ê³„ëŠ” ë°°ì¹˜ë¡œ ì²˜ë¦¬ | Spark Batch + Airflow |
| **ğŸš¨ ì¥ì•  ëŒ€ì‘** | ê° ë ˆì´ì–´ë³„ Fallback ë° Retry ì „ëµ | DLQ, Checkpoint, Retry |
| **ğŸ“ˆ í™•ì¥ì„±** | ìˆ˜í‰ í™•ì¥ ê°€ëŠ¥í•œ ë¶„ì‚° ì•„í‚¤í…ì²˜ | Kafka Cluster, Spark Cluster |

### ë°ì´í„°ì…‹

- **ì¶œì²˜**: [YOOCHOOSE Dataset](https://www.kaggle.com/datasets/chadgostopp/recsys-challenge-2015)
- **ê·œëª¨**: ì•½ 3,300ë§Œ í´ë¦­ ì´ë²¤íŠ¸, 110ë§Œ êµ¬ë§¤ ì´ë²¤íŠ¸
- **ê¸°ê°„**: 2014ë…„ 4ì›” ~ 9ì›” (6ê°œì›”)
- **ë°ì´í„° íƒ€ì…**: 
  - **í´ë¦­ ì´ë²¤íŠ¸**: `session_id`, `timestamp`, `item_id`, `category`
  - **êµ¬ë§¤ ì´ë²¤íŠ¸**: `session_id`, `timestamp`, `item_id`, `price`, `quantity`

---

## âœ¨ ì£¼ìš” ê¸°ëŠ¥

### 1ï¸âƒ£ ì‹¤ì‹œê°„ ì´ìƒ íƒì§€ (Spark Structured Streaming)

#### ê³ ë¹ˆë„ í´ë¦­ íƒì§€ (HIGH_FREQUENCY)
- **ìœˆë„ìš°**: 10ì´ˆ Tumbling Window
- **ì¡°ê±´**: 50íšŒ ì´ìƒ í´ë¦­
- **ëª©ì **: DDoS, í¬ë¡¤ëŸ¬, ì•…ì˜ì  ë´‡ íƒì§€

#### ë´‡ íŒ¨í„´ íƒì§€ (BOT_LIKE)
- **ìœˆë„ìš°**: 1ë¶„ Sliding Window (30ì´ˆ ìŠ¬ë¼ì´ë“œ)
- **ì¡°ê±´**: 100íšŒ ì´ìƒ í´ë¦­ + ê³ ìœ  ì•„ì´í…œ 5ê°œ ì´í•˜
- **ëª©ì **: ìë™í™” ìŠ¤í¬ë¦½íŠ¸, ìƒí’ˆ ì •ì°° ë´‡ íƒì§€

#### ì²˜ë¦¬ íŠ¹ì§•
- âœ… **Micro-batch**: 5ì´ˆ ê°„ê²© ì²˜ë¦¬
- âœ… **Watermark**: 10ì´ˆ ì§€ì—° í—ˆìš© (ëŠ¦ê²Œ ë„ì°©í•œ ë°ì´í„° ì²˜ë¦¬)
- âœ… **Output**: PostgreSQL `anomaly_sessions` í…Œì´ë¸”ì— ì‹¤ì‹œê°„ ì €ì¥
- âœ… **Avro ì—­ì§ë ¬í™”**: Schema Registry ê¸°ë°˜ ìŠ¤í‚¤ë§ˆ ì§„í™” ì§€ì›

### 2ï¸âƒ£ ë°ì´í„° ë ˆì´í¬ (S3/MinIO)

#### Kafka Connect S3 Sink
- **í¬ë§·**: Parquet + Snappy ì••ì¶•
- **íŒŒí‹°ì…”ë‹**: `dt=YYYY-MM-DD/hour=HH` ì‹œê°„ ê¸°ë°˜
- **ëª©ì **: 
  - Raw ë°ì´í„° ì˜êµ¬ ë³´ì¡´
  - ì¬ì²˜ë¦¬(Replay) ê°€ëŠ¥
  - ê·œì • ì¤€ìˆ˜ ë° ê°ì‚¬ ì¶”ì 

#### ì €ì¥ ê²½ë¡œ êµ¬ì¡°
```
s3://km-data-lake/
â”œâ”€â”€ topics/
â”‚   â”œâ”€â”€ km.clicks.raw.v1/
â”‚   â”‚   â””â”€â”€ raw_clicks/
â”‚   â”‚       â”œâ”€â”€ dt=2014-04-07/
â”‚   â”‚       â”‚   â”œâ”€â”€ hour=00/
â”‚   â”‚       â”‚   â”œâ”€â”€ hour=01/
â”‚   â”‚       â”‚   â””â”€â”€ ...
â”‚   â”‚       â””â”€â”€ dt=2014-04-08/
â”‚   â””â”€â”€ km.purchases.raw.v1/
â”‚       â””â”€â”€ raw_purchases/
â”‚           â””â”€â”€ dt=2014-04-07/
```

### 3ï¸âƒ£ ì¼ë³„ ë°°ì¹˜ ë¶„ì„ (Spark Batch + Airflow)

#### Daily Metrics (ì¼ë³„ ë©”íŠ¸ë¦­)
```sql
-- ê³„ì‚° ì§€í‘œ
- ì´ í´ë¦­ ìˆ˜ (total_clicks)
- ì´ êµ¬ë§¤ ìˆ˜ (total_purchases)
- ê³ ìœ  ì„¸ì…˜ ìˆ˜ (unique_sessions)
- ê³ ìœ  ì•„ì´í…œ ìˆ˜ (unique_items)
- ì „í™˜ìœ¨ (conversion_rate)
- í‰ê·  ì„¸ì…˜ ì‹œê°„ (avg_session_duration_sec)
- ì„¸ì…˜ë‹¹ í‰ê·  í´ë¦­ (avg_clicks_per_session)
- ì´ ë§¤ì¶œ (total_revenue)
- í‰ê·  ì£¼ë¬¸ ê¸ˆì•¡ (avg_order_value)
```

#### Session Funnel (ì „í™˜ í¼ë„)
```
ë‹¨ê³„ 1: Single View (1íšŒë§Œ í´ë¦­í•œ ì„¸ì…˜)
  â†“
ë‹¨ê³„ 2: Multi View (ì—¬ëŸ¬ ë²ˆ í´ë¦­í•œ ì„¸ì…˜)
  â†“
ë‹¨ê³„ 3: Purchase (êµ¬ë§¤í•œ ì„¸ì…˜)
```

ê° ë‹¨ê³„ë³„ ì„¸ì…˜ ìˆ˜, ë¹„ìœ¨, ì´íƒˆë¥  ê³„ì‚°

#### Popular Items (ì¸ê¸° ìƒí’ˆ)
- **Top 100 ìƒí’ˆ**: ë§¤ì¶œ ê¸°ì¤€ ìˆœìœ„
- **ì§‘ê³„ ì§€í‘œ**: í´ë¦­ ìˆ˜, êµ¬ë§¤ ìˆ˜, ë§¤ì¶œ, í´ë¦­-êµ¬ë§¤ ì „í™˜ìœ¨
- **ì¹´í…Œê³ ë¦¬ë³„ ì§‘ê³„**: Top 20 ì¹´í…Œê³ ë¦¬

#### Airflow DAG ìŠ¤ì¼€ì¤„ë§
- **ì‹¤í–‰ ì‹œì **: ë§¤ì¼ ìƒˆë²½ 3ì‹œ (KST)
- **ì²˜ë¦¬ ë²”ìœ„**: ì „ë‚ (D-1) ë°ì´í„°
- **ì˜ì¡´ì„±**: daily_metrics â†’ (popular_items + session_funnel)
- **ì¬ì‹œë„**: ì‹¤íŒ¨ ì‹œ 3íšŒ ì¬ì‹œë„, 5ë¶„ ê°„ê²©

### 4ï¸âƒ£ API ì„œë¹™ (FastAPI)

#### RESTful API
- **í”„ë ˆì„ì›Œí¬**: FastAPI (ASGI ë¹„ë™ê¸°)
- **ìë™ ë¬¸ì„œí™”**: Swagger UI (`/docs`), ReDoc (`/redoc`)
- **ì¸ì¦**: í˜„ì¬ ì—†ìŒ (ì¶”í›„ JWT ì¶”ê°€ ê°€ëŠ¥)

#### ì£¼ìš” ì—”ë“œí¬ì¸íŠ¸

**Anomalies (ì´ìƒ íƒì§€)**
```bash
GET /anomalies                    # ìµœê·¼ ì´ìƒ ì„¸ì…˜ ì¡°íšŒ
GET /anomalies/types              # ì´ìƒ ìœ í˜•ë³„ í†µê³„
GET /anomalies/timeline           # ì‹œê°„ëŒ€ë³„ ì´ìƒ ë°œìƒ ì¶”ì´
GET /anomalies/{session_id}       # íŠ¹ì • ì„¸ì…˜ ìƒì„¸ ì¡°íšŒ
```

**Metrics (ì¼ë³„ ë©”íŠ¸ë¦­)**
```bash
GET /metrics/daily                # ì¼ë³„ ë©”íŠ¸ë¦­ ì¡°íšŒ
GET /metrics/daily/{date}         # íŠ¹ì • ë‚ ì§œ ë©”íŠ¸ë¦­
GET /metrics/funnel/{date}        # ì „í™˜ í¼ë„ ë¶„ì„
GET /metrics/summary              # ì „ì²´ ê¸°ê°„ ìš”ì•½ í†µê³„
```

**Items (ìƒí’ˆ ë¶„ì„)**
```bash
GET /items/popular/{date}         # ì¸ê¸° ìƒí’ˆ Top 100
GET /items/categories/{date}      # ì¸ê¸° ì¹´í…Œê³ ë¦¬ Top 20
GET /items/trending               # íŠ¸ë Œë”© ìƒí’ˆ (ìµœê·¼ 3ì¼ ë¹„êµ)
```

### 5ï¸âƒ£ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ (Streamlit)

#### ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ
- **ì´ìƒ íƒì§€ í˜„í™©**: ìµœê·¼ 1ì‹œê°„ ì´ìƒ ì„¸ì…˜ ìˆ˜, ìœ í˜•ë³„ ë¶„í¬
- **ì¼ë³„ ë©”íŠ¸ë¦­**: í´ë¦­/êµ¬ë§¤/ì „í™˜ìœ¨ íŠ¸ë Œë“œ ê·¸ë˜í”„
- **ì„¸ì…˜ í¼ë„**: ë‹¨ê³„ë³„ ì „í™˜ìœ¨ ì‹œê°í™”
- **ì¸ê¸° ìƒí’ˆ**: Top 10 ìƒí’ˆ ë° ì¹´í…Œê³ ë¦¬ ì°¨íŠ¸
- **ìë™ ê°±ì‹ **: 30ì´ˆë§ˆë‹¤ ìë™ ìƒˆë¡œê³ ì¹¨

---

## ğŸ—ï¸ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

### ì „ì²´ ì•„í‚¤í…ì²˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Data Ingestion Layer                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                          â”‚                          â”‚
        â–¼                          â–¼                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CSV Data     â”‚â”€â”€â”€â”€â”€â”€â”€â–¶â”‚   Kafka      â”‚â”€â”€â”€â”€â”€â”€â”€â–¶â”‚   Schema     â”‚
â”‚  Source       â”‚         â”‚  Producers   â”‚         â”‚  Registry    â”‚
â”‚  (YOOCHOOSE)  â”‚         â”‚  (Avro)      â”‚         â”‚  (Avro)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚    Kafka Cluster       â”‚
                    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â” â”‚
                    â”‚ â”‚Brokerâ”‚Brokerâ”‚Brokerâ”‚ â”‚
                    â”‚ â”‚  1   â”‚  2   â”‚  3   â”‚ â”‚
                    â”‚ â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜ â”‚
                    â”‚  Topics:               â”‚
                    â”‚  - km.clicks.raw.v1    â”‚
                    â”‚  - km.purchases.raw.v1 â”‚
                    â”‚  - km.events.dlq.v1    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                    â”‚                    â”‚
        â–¼                    â–¼                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Kafka Connect   â”‚  â”‚ Spark Streaming â”‚  â”‚  Spark Batch    â”‚
â”‚  (S3 Sink)      â”‚  â”‚ (Anomaly)       â”‚  â”‚  (Analytics)    â”‚
â”‚                 â”‚  â”‚                 â”‚  â”‚                 â”‚
â”‚ â€¢ Time-based    â”‚  â”‚ â€¢ Tumbling      â”‚  â”‚ â€¢ Airflow DAG   â”‚
â”‚   partitioning  â”‚  â”‚ â€¢ Sliding       â”‚  â”‚ â€¢ S3 â†’ Parquet  â”‚
â”‚ â€¢ Parquet       â”‚  â”‚ â€¢ Watermark     â”‚  â”‚ â€¢ Aggregation   â”‚
â”‚ â€¢ Snappy        â”‚  â”‚ â€¢ Micro-batch   â”‚  â”‚ â€¢ Join          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                    â”‚                    â”‚
         â–¼                    â–¼                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   S3/MinIO      â”‚  â”‚   PostgreSQL    â”‚  â”‚   PostgreSQL    â”‚
â”‚  Data Lake      â”‚  â”‚  (Anomaly DB)   â”‚  â”‚  (Metrics DB)   â”‚
â”‚                 â”‚  â”‚                 â”‚  â”‚                 â”‚
â”‚ â€¢ Raw Data      â”‚  â”‚ â€¢ anomaly_      â”‚  â”‚ â€¢ daily_metrics â”‚
â”‚ â€¢ Immutable     â”‚  â”‚   sessions      â”‚  â”‚ â€¢ popular_items â”‚
â”‚ â€¢ Replay        â”‚  â”‚ â€¢ Real-time     â”‚  â”‚ â€¢ session_funnelâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚                    â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                         â”‚
                                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Serving Layer                               â”‚
â”‚                                                                    â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚   â”‚    FastAPI        â”‚            â”‚   Streamlit       â”‚           â”‚
â”‚   â”‚    (REST API)     â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚   (Dashboard)     â”‚           â”‚
â”‚   â”‚                   â”‚            â”‚                   â”‚           â”‚
â”‚   â”‚ â€¢ /anomalies      â”‚            â”‚ â€¢ Real-time view  â”‚           â”‚
â”‚   â”‚ â€¢ /metrics        â”‚            â”‚ â€¢ Charts          â”‚           â”‚
â”‚   â”‚ â€¢ /items          â”‚            â”‚ â€¢ Auto-refresh    â”‚           â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ë°ì´í„° íë¦„ (Data Flow)

#### ì‹¤ì‹œê°„ ê²½ë¡œ (Speed Layer)
```
CSV Data â†’ Kafka Producer â†’ Kafka Cluster â†’ Spark Streaming â†’ PostgreSQL â†’ API/Dashboard
                                                  (5ì´ˆ ë§ˆì´í¬ë¡œë°°ì¹˜)
```

#### ë°°ì¹˜ ê²½ë¡œ (Batch Layer)
```
CSV Data â†’ Kafka Producer â†’ Kafka Cluster â†’ Kafka Connect â†’ S3 Data Lake
                                              (ì‹¤ì‹œê°„ ì ì¬)
                                                     â†“
                                              Spark Batch (Airflow)
                                              (ë§¤ì¼ ìƒˆë²½ 3ì‹œ)
                                                     â†“
                                              PostgreSQL â†’ API/Dashboard
```

---

## ğŸ› ï¸ ê¸°ìˆ  ìŠ¤íƒ

### ë°ì´í„° ì²˜ë¦¬

| ê¸°ìˆ  | ë²„ì „ | ìš©ë„ |
|------|------|------|
| **Apache Kafka** | 3.5.0 | ë¶„ì‚° ë©”ì‹œì§€ í, ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¬ë° |
| **Confluent Schema Registry** | 7.5.0 | Avro ìŠ¤í‚¤ë§ˆ ê´€ë¦¬ ë° ì§„í™” |
| **Kafka Connect** | 7.5.0 | S3 Sink Connector |
| **Apache Spark** | 3.5.0 | ìŠ¤íŠ¸ë¦¬ë° ë° ë°°ì¹˜ ë°ì´í„° ì²˜ë¦¬ |
| **Apache Airflow** | 2.8.0 | ì›Œí¬í”Œë¡œìš° ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ |

### ì €ì¥ì†Œ

| ê¸°ìˆ  | ë²„ì „ | ìš©ë„ |
|------|------|------|
| **MinIO** | latest | S3 í˜¸í™˜ ê°ì²´ ìŠ¤í† ë¦¬ì§€ (Data Lake) |
| **PostgreSQL** | 15 | OLTP ë°ì´í„°ë² ì´ìŠ¤ (ë©”íŠ¸ë¦­, ì´ìƒ íƒì§€) |
| **Zookeeper** | 3.5.0 | Kafka í´ëŸ¬ìŠ¤í„° ì½”ë””ë„¤ì´ì…˜ |

### ì• í”Œë¦¬ì¼€ì´ì…˜

| ê¸°ìˆ  | ë²„ì „ | ìš©ë„ |
|------|------|------|
| **FastAPI** | 0.109.0 | RESTful API ì„œë²„ |
| **Uvicorn** | 0.27.0 | ASGI ì›¹ ì„œë²„ |
| **Streamlit** | 1.30.0 | ì¸í„°ë™í‹°ë¸Œ ëŒ€ì‹œë³´ë“œ |

### ì§ë ¬í™” ë° í¬ë§·

| ê¸°ìˆ  | ìš©ë„ |
|------|------|
| **Apache Avro** | ìŠ¤í‚¤ë§ˆ ê¸°ë°˜ ì§ë ¬í™” (Kafka ë©”ì‹œì§€) |
| **Parquet** | ì»¬ëŸ¼í˜• ìŠ¤í† ë¦¬ì§€ í¬ë§· (S3) |
| **Snappy** | ì••ì¶• ì•Œê³ ë¦¬ì¦˜ |

### ì¸í”„ë¼

| ê¸°ìˆ  | ë²„ì „ | ìš©ë„ |
|------|------|------|
| **Docker** | 24.0+ | ì»¨í…Œì´ë„ˆí™” |
| **Docker Compose** | 2.0+ | ë©€í‹° ì»¨í…Œì´ë„ˆ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ |

### ì–¸ì–´

| ì–¸ì–´ | ë²„ì „ | ìš©ë„ |
|------|------|------|
| **Python** | 3.11 | ë©”ì¸ ê°œë°œ ì–¸ì–´ |
| **SQL** | - | ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬ |
| **Bash** | - | ìŠ¤í¬ë¦½íŠ¸ ìë™í™” |

---

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### ì‚¬ì „ ìš”êµ¬ì‚¬í•­

#### í•„ìˆ˜
- **Docker Desktop** (Windows/Mac) ë˜ëŠ” **Docker + Docker Compose** (Linux)
- **Python** 3.11 ì´ìƒ
- **Git**

#### ê¶Œì¥ ì‚¬ì–‘
- **RAM**: ìµœì†Œ 16GB (ê¶Œì¥ 32GB)
- **ë””ìŠ¤í¬**: ìµœì†Œ 50GB ì—¬ìœ  ê³µê°„
- **CPU**: 4ì½”ì–´ ì´ìƒ

### 1ë‹¨ê³„: í”„ë¡œì íŠ¸ í´ë¡ 

```bash
git clone https://github.com/yourusername/Clickstream-Guardian.git
cd Clickstream-Guardian
```

### 2ë‹¨ê³„: ì „ì²´ ì¸í”„ë¼ ì‹œì‘

```bash
cd docker
docker-compose up -d
```

**ì»¨í…Œì´ë„ˆ ì‹œì‘ ìˆœì„œ (ìë™)**:
1. Zookeeper
2. Kafka Brokers (kafka-1, kafka-2, kafka-3)
3. Schema Registry
4. Kafka Connect
5. PostgreSQL
6. MinIO
7. Spark Master + Workers
8. Airflow Webserver + Scheduler
9. API Server
10. Dashboard

**ëŒ€ê¸° ì‹œê°„**: ì•½ 2-3ë¶„ (ëª¨ë“  ì„œë¹„ìŠ¤ê°€ ì¤€ë¹„ë  ë•Œê¹Œì§€)

### 3ë‹¨ê³„: ì´ˆê¸° ì„¤ì •

#### 3.1 Kafka í† í”½ ìƒì„±

```bash
# í† í”½ ìƒì„±
docker exec kafka-1 kafka-topics --create \
  --bootstrap-server kafka-1:29092 \
  --topic km.clicks.raw.v1 \
  --partitions 3 \
  --replication-factor 2

docker exec kafka-1 kafka-topics --create \
  --bootstrap-server kafka-1:29092 \
  --topic km.purchases.raw.v1 \
  --partitions 3 \
  --replication-factor 2

docker exec kafka-1 kafka-topics --create \
  --bootstrap-server kafka-1:29092 \
  --topic km.events.dlq.v1 \
  --partitions 1 \
  --replication-factor 2

# í† í”½ í™•ì¸
docker exec kafka-1 kafka-topics --list --bootstrap-server kafka-1:29092
```

#### 3.2 Avro ìŠ¤í‚¤ë§ˆ ë“±ë¡

```bash
cd producers
python common/schema_registry.py http://localhost:8081 ../schemas
```

**ì¶œë ¥ ì˜ˆì‹œ**:
```
âœ… Registered click-event.avsc with ID: 1
âœ… Registered purchase-event.avsc with ID: 2
âœ… Registered anomaly-event.avsc with ID: 3
```

#### 3.3 Kafka Connect ì„¤ì •

```bash
cd ../scripts
bash setup_connectors.sh
```

**S3 Sink Connectors**:
- `s3-sink-clicks`: í´ë¦­ ì´ë²¤íŠ¸ â†’ S3
- `s3-sink-purchases`: êµ¬ë§¤ ì´ë²¤íŠ¸ â†’ S3

### 4ë‹¨ê³„: ë°ì´í„° í”„ë¡œë“€ì‹±

#### í´ë¦­ ì´ë²¤íŠ¸

```bash
cd ../producers

# ìƒ˜í”Œ 10,000ê±´
python producer_clicks.py --max-events 10000

# ì „ì²´ ë°ì´í„° (ì•½ 3,300ë§Œ ê±´)
python producer_clicks.py

# ë¹ ë¥¸ ì†ë„ë¡œ (100ë°°ì†)
python producer_clicks.py --replay-speed 100.0
```

#### êµ¬ë§¤ ì´ë²¤íŠ¸

```bash
# ìƒ˜í”Œ 1,000ê±´
python producer_purchases.py --max-events 1000

# ì „ì²´ ë°ì´í„° (ì•½ 110ë§Œ ê±´)
python producer_purchases.py
```

**ì˜µì…˜**:
- `--max-events N`: ìµœëŒ€ Nê°œ ì´ë²¤íŠ¸ë§Œ ì „ì†¡
- `--replay-speed X`: Xë°°ì†ìœ¼ë¡œ ì¬ìƒ (ê¸°ë³¸ 1.0)
- `--anomaly-interval N`: Nì´ˆë§ˆë‹¤ ì´ìƒ íŒ¨í„´ ì£¼ì… (í…ŒìŠ¤íŠ¸ìš©)

### 5ë‹¨ê³„: Spark Streaming ì‹œì‘

```bash
docker exec spark-master spark-submit \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,\
              org.apache.spark:spark-avro_2.12:3.5.0,\
              org.postgresql:postgresql:42.6.0 \
  --master spark://spark-master:7077 \
  --executor-memory 2g \
  --executor-cores 2 \
  /opt/spark-streaming/anomaly_detector.py
```

**í™•ì¸**:
- Spark UI: http://localhost:4040
- PostgreSQLì— ë°ì´í„° í™•ì¸:
  ```bash
  docker exec -it postgres psql -U admin -d clickstream -c "SELECT COUNT(*) FROM anomaly_sessions;"
  ```

### 6ë‹¨ê³„: ë°°ì¹˜ ë¶„ì„ ì‹¤í–‰

#### ìˆ˜ë™ ì‹¤í–‰ (íŠ¹ì • ë‚ ì§œ)

```bash
# ì¼ë³„ ë©”íŠ¸ë¦­
docker exec spark-master spark-submit \
  --packages org.postgresql:postgresql:42.6.0,\
              org.apache.hadoop:hadoop-aws:3.3.4,\
              com.amazonaws:aws-java-sdk-bundle:1.12.262 \
  --master spark://spark-master:7077 \
  /opt/spark-batch/daily_metrics.py 2014-04-07

# ì„¸ì…˜ í¼ë„
docker exec spark-master spark-submit \
  --packages org.postgresql:postgresql:42.6.0,\
              org.apache.hadoop:hadoop-aws:3.3.4,\
              com.amazonaws:aws-java-sdk-bundle:1.12.262 \
  --master spark://spark-master:7077 \
  /opt/spark-batch/session_funnel.py 2014-04-07

# ì¸ê¸° ìƒí’ˆ
docker exec spark-master spark-submit \
  --packages org.postgresql:postgresql:42.6.0,\
              org.apache.hadoop:hadoop-aws:3.3.4,\
              com.amazonaws:aws-java-sdk-bundle:1.12.262 \
  --master spark://spark-master:7077 \
  /opt/spark-batch/popular_items.py 2014-04-07
```

#### Airflow DAG ì‹¤í–‰

1. Airflow UI ì ‘ì†: http://localhost:8082
2. ë¡œê·¸ì¸: `admin` / `admin`
3. DAG `daily_batch_pipeline` í™œì„±í™”
4. ìˆ˜ë™ íŠ¸ë¦¬ê±°: `Trigger DAG` ë²„íŠ¼ í´ë¦­

### 7ë‹¨ê³„: ì„œë¹„ìŠ¤ ì ‘ì†

| ì„œë¹„ìŠ¤ | URL | ì¸ì¦ ì •ë³´ |
|---------|-----|-----------|
| **Dashboard** | http://localhost:8501 | - |
| **API Swagger UI** | http://localhost:8000/docs | - |
| **Airflow** | http://localhost:8082 | admin / admin |
| **MinIO Console** | http://localhost:9001 | minioadmin / minioadmin |
| **Spark Master UI** | http://localhost:8080 | - |
| **Spark Streaming UI** | http://localhost:4040 | - |

---

## ğŸ“š ìƒì„¸ ì‚¬ìš© ê°€ì´ë“œ

### Kafka Producer ì‚¬ìš©ë²•

#### ê¸°ë³¸ ì‚¬ìš©

```bash
# í´ë¦­ ì´ë²¤íŠ¸ ì „ì†¡
python producer_clicks.py

# êµ¬ë§¤ ì´ë²¤íŠ¸ ì „ì†¡
python producer_purchases.py
```

#### ê³ ê¸‰ ì˜µì…˜

```bash
# ìµœëŒ€ 10,000ê°œ ì´ë²¤íŠ¸ë§Œ ì „ì†¡
python producer_clicks.py --max-events 10000

# 100ë°°ì†ìœ¼ë¡œ ì¬ìƒ
python producer_clicks.py --replay-speed 100.0

# 60ì´ˆë§ˆë‹¤ ì´ìƒ íŒ¨í„´ ì£¼ì…
python producer_clicks.py --anomaly-interval 60

# CSV íŒŒì¼ ê²½ë¡œ ì§€ì •
python producer_clicks.py --csv-path ../data/yoochoose-clicks-sorted.dat
```

#### DLQ í…ŒìŠ¤íŠ¸

```bash
# ì˜ë„ì ìœ¼ë¡œ ì˜ëª»ëœ ìŠ¤í‚¤ë§ˆ ì „ì†¡
python producer_dlq_real_demo.py
```

### Spark Batch ì‘ì—… ì‹¤í–‰

#### 1. Daily Metrics

```bash
docker exec spark-master spark-submit \
  --packages org.postgresql:postgresql:42.6.0,\
              org.apache.hadoop:hadoop-aws:3.3.4,\
              com.amazonaws:aws-java-sdk-bundle:1.12.262 \
  --master spark://spark-master:7077 \
  --executor-memory 4g \
  --executor-cores 2 \
  /opt/spark-batch/daily_metrics.py 2014-04-07
```

**ì¶œë ¥**:
```
Processing date: 2014-04-07
âœ… Loaded 1,234,567 clicks
âœ… Loaded 12,345 purchases
âœ… Calculated daily metrics
âœ… Written to PostgreSQL: daily_metrics table
```

#### 2. Session Funnel

```bash
docker exec spark-master spark-submit \
  --packages org.postgresql:postgresql:42.6.0,\
              org.apache.hadoop:hadoop-aws:3.3.4,\
              com.amazonaws:aws-java-sdk-bundle:1.12.262 \
  --master spark://spark-master:7077 \
  /opt/spark-batch/session_funnel.py 2014-04-07
```

#### 3. Popular Items

```bash
docker exec spark-master spark-submit \
  --packages org.postgresql:postgresql:42.6.0,\
              org.apache.hadoop:hadoop-aws:3.3.4,\
              com.amazonaws:aws-java-sdk-bundle:1.12.262 \
  --master spark://spark-master:7077 \
  /opt/spark-batch/popular_items.py 2014-04-07
```

### API ì‚¬ìš© ì˜ˆì œ

#### cURL

```bash
# í—¬ìŠ¤ ì²´í¬
curl http://localhost:8000/health

# ìµœê·¼ ì´ìƒ ì„¸ì…˜ 10ê°œ ì¡°íšŒ
curl "http://localhost:8000/anomalies?limit=10"

# íŠ¹ì • ë‚ ì§œ ì¼ë³„ ë©”íŠ¸ë¦­
curl "http://localhost:8000/metrics/daily/2014-04-07"

# ë‚ ì§œ ë²”ìœ„ ì¡°íšŒ
curl "http://localhost:8000/metrics/daily?start_date=2014-04-07&end_date=2014-04-10"

# ì¸ê¸° ìƒí’ˆ Top 50
curl "http://localhost:8000/items/popular/2014-04-07?limit=50"

# ì¸ê¸° ì¹´í…Œê³ ë¦¬
curl "http://localhost:8000/items/categories/2014-04-07"

# ì „í™˜ í¼ë„
curl "http://localhost:8000/metrics/funnel/2014-04-07"
```

#### Python (requests)

```python
import requests

# API ê¸°ë³¸ URL
BASE_URL = "http://localhost:8000"

# ì´ìƒ ì„¸ì…˜ ì¡°íšŒ
response = requests.get(f"{BASE_URL}/anomalies", params={"limit": 10})
anomalies = response.json()

for anomaly in anomalies:
    print(f"Session {anomaly['session_id']}: {anomaly['anomaly_type']}")

# ì¼ë³„ ë©”íŠ¸ë¦­ ì¡°íšŒ
response = requests.get(f"{BASE_URL}/metrics/daily/2014-04-07")
metrics = response.json()

print(f"Total clicks: {metrics['total_clicks']}")
print(f"Conversion rate: {metrics['conversion_rate']:.2%}")
```

### S3 ë°ì´í„° í™•ì¸

#### MinIO Console

1. http://localhost:9001 ì ‘ì†
2. ë¡œê·¸ì¸: `minioadmin` / `minioadmin`
3. Buckets â†’ `km-data-lake` ì„ íƒ

#### AWS CLI (S3 í˜¸í™˜)

```bash
# MinIOì— S3 CLI ì—°ê²°
aws configure set aws_access_key_id minioadmin
aws configure set aws_secret_access_key minioadmin

# ë²„í‚· ëª©ë¡
aws s3 ls --endpoint-url http://localhost:9000

# íŒŒì¼ ëª©ë¡
aws s3 ls s3://km-data-lake/topics/km.clicks.raw.v1/raw_clicks/ \
  --endpoint-url http://localhost:9000 --recursive

# íŒŒì¼ ë‹¤ìš´ë¡œë“œ
aws s3 cp s3://km-data-lake/topics/km.clicks.raw.v1/raw_clicks/dt=2014-04-07/hour=00/file.parquet \
  . --endpoint-url http://localhost:9000
```

### PostgreSQL ë°ì´í„° ì¡°íšŒ

```bash
# PostgreSQL ì ‘ì†
docker exec -it postgres psql -U admin -d clickstream

# ì´ìƒ ì„¸ì…˜ í™•ì¸
SELECT * FROM anomaly_sessions ORDER BY detected_at DESC LIMIT 10;

# ì¼ë³„ ë©”íŠ¸ë¦­ í™•ì¸
SELECT * FROM daily_metrics ORDER BY metric_date DESC;

# ì¸ê¸° ìƒí’ˆ í™•ì¸
SELECT * FROM popular_items WHERE metric_date = '2014-04-07' ORDER BY rank LIMIT 10;

# ì„¸ì…˜ í¼ë„ í™•ì¸
SELECT * FROM session_funnel WHERE metric_date = '2014-04-07';
```

---

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
Clickstream-Guardian/
â”‚
â”œâ”€â”€ ğŸ“„ README.md                    # í”„ë¡œì íŠ¸ ë©”ì¸ ë¬¸ì„œ
â”œâ”€â”€ ğŸ“„ requirements.txt             # Python ì˜ì¡´ì„±
â”œâ”€â”€ ğŸ“„ .gitignore                   # Git ì œì™¸ íŒŒì¼
â”œâ”€â”€ ğŸ““ test.ipynb                   # Jupyter ë…¸íŠ¸ë¶ (ë°ì´í„° íƒìƒ‰)
â”‚
â”œâ”€â”€ ğŸ“‚ data/                        # ì›ë³¸ ë°ì´í„°
â”‚   â”œâ”€â”€ dataset-README.txt         # ë°ì´í„°ì…‹ ì„¤ëª…
â”‚   â”œâ”€â”€ preprocess_data.py         # ì „ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸
â”‚   â”œâ”€â”€ yoochoose-clicks-sorted.dat     # í´ë¦­ ë¡œê·¸ (3,300ë§Œ ê±´)
â”‚   â””â”€â”€ yoochoose-buys-sorted.dat       # êµ¬ë§¤ ë¡œê·¸ (110ë§Œ ê±´)
â”‚
â”œâ”€â”€ ğŸ“‚ schemas/                     # Avro ìŠ¤í‚¤ë§ˆ ì •ì˜
â”‚   â”œâ”€â”€ click-event.avsc           # í´ë¦­ ì´ë²¤íŠ¸ ìŠ¤í‚¤ë§ˆ
â”‚   â”œâ”€â”€ purchase-event.avsc        # êµ¬ë§¤ ì´ë²¤íŠ¸ ìŠ¤í‚¤ë§ˆ
â”‚   â””â”€â”€ anomaly-event.avsc         # ì´ìƒ ì´ë²¤íŠ¸ ìŠ¤í‚¤ë§ˆ
â”‚
â”œâ”€â”€ ğŸ“‚ producers/                   # Kafka Producer
â”‚   â”œâ”€â”€ producer_clicks.py         # í´ë¦­ ì´ë²¤íŠ¸ í”„ë¡œë“€ì„œ
â”‚   â”œâ”€â”€ producer_purchases.py      # êµ¬ë§¤ ì´ë²¤íŠ¸ í”„ë¡œë“€ì„œ
â”‚   â”œâ”€â”€ producer_dlq_real_demo.py  # DLQ í…ŒìŠ¤íŠ¸ìš© í”„ë¡œë“€ì„œ
â”‚   â”œâ”€â”€ requirements.txt           # Python ì˜ì¡´ì„±
â”‚   â””â”€â”€ common/                    # ê³µí†µ ëª¨ë“ˆ
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ config.py              # ì„¤ì • (Kafka, Schema Registry)
â”‚       â””â”€â”€ schema_registry.py     # ìŠ¤í‚¤ë§ˆ ë“±ë¡ ìœ í‹¸ë¦¬í‹°
â”‚
â”œâ”€â”€ ğŸ“‚ connectors/                  # Kafka Connect ì„¤ì •
â”‚   â”œâ”€â”€ s3-sink-clicks.json        # í´ë¦­ â†’ S3 Sink
â”‚   â””â”€â”€ s3-sink-purchases.json     # êµ¬ë§¤ â†’ S3 Sink
â”‚
â”œâ”€â”€ ğŸ“‚ spark-streaming/             # Spark Structured Streaming
â”‚   â”œâ”€â”€ anomaly_detector.py        # ì´ìƒ íƒì§€ ë©”ì¸ ë¡œì§
â”‚   â”œâ”€â”€ requirements.txt           # Python ì˜ì¡´ì„±
â”‚   â””â”€â”€ common/                    # ê³µí†µ ëª¨ë“ˆ
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ kafka_utils.py         # Kafka ì½ê¸° ìœ í‹¸ë¦¬í‹°
â”‚       â””â”€â”€ postgres_utils.py      # PostgreSQL ì“°ê¸° ìœ í‹¸ë¦¬í‹°
â”‚
â”œâ”€â”€ ğŸ“‚ spark-batch/                 # Spark Batch ì‘ì—…
â”‚   â”œâ”€â”€ daily_metrics.py           # ì¼ë³„ ë©”íŠ¸ë¦­ ì§‘ê³„
â”‚   â”œâ”€â”€ session_funnel.py          # ì „í™˜ í¼ë„ ë¶„ì„
â”‚   â”œâ”€â”€ popular_items.py           # ì¸ê¸° ìƒí’ˆ ë¶„ì„
â”‚   â”œâ”€â”€ requirements.txt           # Python ì˜ì¡´ì„±
â”‚   â””â”€â”€ common/                    # ê³µí†µ ëª¨ë“ˆ
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ s3_utils.py            # S3 ì½ê¸° ìœ í‹¸ë¦¬í‹°
â”‚
â”œâ”€â”€ ğŸ“‚ airflow/                     # Apache Airflow
â”‚   â”œâ”€â”€ dags/                      # DAG ì •ì˜
â”‚   â”‚   â””â”€â”€ spark_batch_dag.py    # ì¼ë³„ ë°°ì¹˜ DAG
â”‚   â””â”€â”€ plugins/                   # ì»¤ìŠ¤í…€ í”ŒëŸ¬ê·¸ì¸ (ë¹„ì–´ìˆìŒ)
â”‚
â”œâ”€â”€ ğŸ“‚ api/                         # FastAPI ì„œë²„
â”‚   â”œâ”€â”€ main.py                    # FastAPI ì•± ì§„ì…ì 
â”‚   â”œâ”€â”€ config.py                  # ì„¤ì • (DB URL ë“±)
â”‚   â”œâ”€â”€ requirements.txt           # Python ì˜ì¡´ì„±
â”‚   â”œâ”€â”€ Dockerfile                 # API ì»¨í…Œì´ë„ˆ ì´ë¯¸ì§€
â”‚   â”œâ”€â”€ models/                    # ë°ì´í„°ë² ì´ìŠ¤ ëª¨ë¸
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ database.py           # DB ì—°ê²° ë° ì„¸ì…˜ ê´€ë¦¬
â”‚   â””â”€â”€ routers/                   # API ë¼ìš°í„°
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ anomaly.py            # /anomalies ì—”ë“œí¬ì¸íŠ¸
â”‚       â”œâ”€â”€ metrics.py            # /metrics ì—”ë“œí¬ì¸íŠ¸
â”‚       â””â”€â”€ sessions.py           # /items ì—”ë“œí¬ì¸íŠ¸
â”‚
â”œâ”€â”€ ğŸ“‚ dashboard/                   # Streamlit ëŒ€ì‹œë³´ë“œ
â”‚   â”œâ”€â”€ app.py                     # ëŒ€ì‹œë³´ë“œ ë©”ì¸ ì•±
â”‚   â”œâ”€â”€ requirements.txt           # Python ì˜ì¡´ì„±
â”‚   â””â”€â”€ Dockerfile                 # Dashboard ì»¨í…Œì´ë„ˆ ì´ë¯¸ì§€
â”‚
â”œâ”€â”€ ğŸ“‚ docker/                      # Docker ì„¤ì •
â”‚   â”œâ”€â”€ docker-compose.yml         # ì „ì²´ ì¸í”„ë¼ ì •ì˜
â”‚   â”œâ”€â”€ airflow/                   # Airflow ì»¨í…Œì´ë„ˆ
â”‚   â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”‚   â””â”€â”€ entrypoint.sh
â”‚   â”œâ”€â”€ spark/                     # Spark ì»¨í…Œì´ë„ˆ
â”‚   â”‚   â””â”€â”€ Dockerfile
â”‚   â”œâ”€â”€ postgres/                  # PostgreSQL ì´ˆê¸°í™”
â”‚   â”‚   â””â”€â”€ init.sql              # í…Œì´ë¸” ìƒì„± SQL
â”‚   â”œâ”€â”€ connectors/                # Kafka Connect ì´ˆê¸°í™”
â”‚   â”‚   â””â”€â”€ init-connectors.sh    # Connector ë“±ë¡ ìŠ¤í¬ë¦½íŠ¸
â”‚   â”œâ”€â”€ spark-batch/               # Spark Batch ì‘ì—… ë§ˆìš´íŠ¸
â”‚   â””â”€â”€ spark-streaming/           # Spark Streaming ì‘ì—… ë§ˆìš´íŠ¸
â”‚
â”œâ”€â”€ ğŸ“‚ scripts/                     # ìœ í‹¸ë¦¬í‹° ìŠ¤í¬ë¦½íŠ¸
â”‚   â”œâ”€â”€ setup_connectors.sh        # Kafka Connect ì„¤ì •
â”‚   â”œâ”€â”€ reset-pipeline.sh          # íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
â”‚   â”œâ”€â”€ test_batch_dag.sh          # DAG í…ŒìŠ¤íŠ¸
â”‚   â””â”€â”€ load_daily_data_to_s3.py   # S3ì— ë°ì´í„° ì§ì ‘ ë¡œë“œ
â”‚
â”œâ”€â”€ ğŸ“‚ docs/                        # ìƒì„¸ ë¬¸ì„œ
â”‚   â”œâ”€â”€ OPERATIONS_README.md       # ìš´ì˜ ë¬¸ì„œ ì¸ë±ìŠ¤
â”‚   â”œâ”€â”€ KAFKA_DESIGN.md            # Kafka ì„¤ê³„ ë¬¸ì„œ
â”‚   â”œâ”€â”€ SPARK_STREAMING_DESIGN.md  # Spark Streaming ì„¤ê³„
â”‚   â”œâ”€â”€ AIRFLOW_DAG_DESIGN.md      # Airflow DAG ì„¤ê³„
â”‚   â”œâ”€â”€ API_SERVING_DESIGN.md      # API ì„¤ê³„ ë¬¸ì„œ
â”‚   â”œâ”€â”€ MONITORING_STRATEGY.md     # ëª¨ë‹ˆí„°ë§ ì „ëµ
â”‚   â”œâ”€â”€ FAILURE_RECOVERY_STRATEGY.md  # ì¥ì•  ë³µêµ¬ ì „ëµ
â”‚   â”œâ”€â”€ FALLBACK_ALERT_STRATEGY.md    # Fallback ë° ì•Œë¦¼
â”‚   â””â”€â”€ LOAD_TEST_SCENARIO.md         # ë¶€í•˜ í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤
â”‚
â””â”€â”€ ğŸ“‚ diagram/                     # ì•„í‚¤í…ì²˜ ë‹¤ì´ì–´ê·¸ë¨
    â””â”€â”€ architecture.png           # ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ ì´ë¯¸ì§€
```

---

## ğŸ“– API ë¬¸ì„œ

FastAPIëŠ” **ìë™ìœ¼ë¡œ OpenAPI(Swagger) ë¬¸ì„œ**ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

### ë¬¸ì„œ ì ‘ì†

- **Swagger UI**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc

### ì£¼ìš” ì—”ë“œí¬ì¸íŠ¸

#### 1. System Endpoints

##### `GET /`
ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸, API ì •ë³´ ë°˜í™˜

**ì‘ë‹µ ì˜ˆì‹œ**:
```json
{
  "message": "Clickstream Guardian API",
  "version": "1.0.0",
  "docs": "/docs",
  "redoc": "/redoc"
}
```

##### `GET /health`
í—¬ìŠ¤ ì²´í¬, DB ì—°ê²° ìƒíƒœ í™•ì¸

**ì‘ë‹µ ì˜ˆì‹œ**:
```json
{
  "status": "healthy",
  "database": "connected",
  "version": "1.0.0"
}
```

##### `GET /stats`
ì „ì²´ ì‹œìŠ¤í…œ í†µê³„

**ì‘ë‹µ ì˜ˆì‹œ**:
```json
{
  "total_anomalies": 1523,
  "days_processed": 5,
  "total_clicks": 12345678,
  "total_purchases": 123456,
  "total_sessions": 234567
}
```

#### 2. Anomalies Endpoints

##### `GET /anomalies`
ìµœê·¼ ì´ìƒ ì„¸ì…˜ ì¡°íšŒ

**Query Parameters**:
- `limit` (int, optional): ë°˜í™˜í•  ìµœëŒ€ ê°œìˆ˜ (default: 100)
- `anomaly_type` (str, optional): ì´ìƒ ìœ í˜• í•„í„° (`HIGH_FREQUENCY`, `BOT_LIKE`)

**ì‘ë‹µ ì˜ˆì‹œ**:
```json
[
  {
    "session_id": 12345,
    "anomaly_type": "HIGH_FREQUENCY",
    "click_count": 52,
    "unique_items": 15,
    "window_start": "2014-04-07T10:15:00",
    "window_end": "2014-04-07T10:15:10",
    "detected_at": "2014-04-07T10:15:11"
  }
]
```

##### `GET /anomalies/types`
ì´ìƒ ìœ í˜•ë³„ í†µê³„

**ì‘ë‹µ ì˜ˆì‹œ**:
```json
[
  {
    "anomaly_type": "HIGH_FREQUENCY",
    "count": 856
  },
  {
    "anomaly_type": "BOT_LIKE",
    "count": 667
  }
]
```

##### `GET /anomalies/timeline`
ì‹œê°„ëŒ€ë³„ ì´ìƒ ë°œìƒ ì¶”ì´ (1ì‹œê°„ ë‹¨ìœ„)

**Query Parameters**:
- `hours` (int, optional): ì¡°íšŒí•  ì‹œê°„ ë²”ìœ„ (default: 24)

**ì‘ë‹µ ì˜ˆì‹œ**:
```json
[
  {
    "hour": "2014-04-07T10:00:00",
    "count": 45
  },
  {
    "hour": "2014-04-07T11:00:00",
    "count": 52
  }
]
```

##### `GET /anomalies/{session_id}`
íŠ¹ì • ì„¸ì…˜ì˜ ëª¨ë“  ì´ìƒ ì¡°íšŒ

**Path Parameters**:
- `session_id` (int): ì„¸ì…˜ ID

**ì‘ë‹µ ì˜ˆì‹œ**:
```json
[
  {
    "session_id": 12345,
    "anomaly_type": "HIGH_FREQUENCY",
    "click_count": 52,
    "unique_items": 15,
    "window_start": "2014-04-07T10:15:00",
    "window_end": "2014-04-07T10:15:10",
    "detected_at": "2014-04-07T10:15:11"
  }
]
```

#### 3. Metrics Endpoints

##### `GET /metrics/daily`
ì¼ë³„ ë©”íŠ¸ë¦­ ì¡°íšŒ

**Query Parameters**:
- `start_date` (str, optional): ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD)
- `end_date` (str, optional): ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD)
- `limit` (int, optional): ë°˜í™˜í•  ìµœëŒ€ ê°œìˆ˜ (default: 30)

**ì‘ë‹µ ì˜ˆì‹œ**:
```json
[
  {
    "metric_date": "2014-04-07",
    "total_clicks": 1234567,
    "total_purchases": 12345,
    "unique_sessions": 23456,
    "unique_items": 5678,
    "conversion_rate": 0.0526,
    "avg_session_duration_sec": 345.67,
    "avg_clicks_per_session": 52.63,
    "total_revenue": 123456.78,
    "avg_order_value": 10.00
  }
]
```

##### `GET /metrics/daily/{date}`
íŠ¹ì • ë‚ ì§œì˜ ë©”íŠ¸ë¦­

**Path Parameters**:
- `date` (str): ë‚ ì§œ (YYYY-MM-DD)

##### `GET /metrics/funnel/{date}`
ì „í™˜ í¼ë„ ë¶„ì„

**Path Parameters**:
- `date` (str): ë‚ ì§œ (YYYY-MM-DD)

**ì‘ë‹µ ì˜ˆì‹œ**:
```json
[
  {
    "metric_date": "2014-04-07",
    "funnel_stage": "Single View",
    "session_count": 15000,
    "percentage": 64.10,
    "drop_rate": 0.0
  },
  {
    "metric_date": "2014-04-07",
    "funnel_stage": "Multi View",
    "session_count": 7000,
    "percentage": 29.91,
    "drop_rate": 53.33
  },
  {
    "metric_date": "2014-04-07",
    "funnel_stage": "Purchase",
    "session_count": 1400,
    "percentage": 5.98,
    "drop_rate": 80.00
  }
]
```

##### `GET /metrics/summary`
ì „ì²´ ê¸°ê°„ ìš”ì•½ í†µê³„

**ì‘ë‹µ ì˜ˆì‹œ**:
```json
{
  "total_clicks": 33003944,
  "total_purchases": 1150753,
  "unique_sessions": 9249729,
  "conversion_rate": 0.0349,
  "avg_order_value": 12.45,
  "total_revenue": 14329879.85
}
```

#### 4. Items Endpoints

##### `GET /items/popular/{date}`
ì¸ê¸° ìƒí’ˆ Top 100

**Path Parameters**:
- `date` (str): ë‚ ì§œ (YYYY-MM-DD)

**Query Parameters**:
- `limit` (int, optional): ë°˜í™˜í•  ìµœëŒ€ ê°œìˆ˜ (default: 100)

**ì‘ë‹µ ì˜ˆì‹œ**:
```json
[
  {
    "metric_date": "2014-04-07",
    "item_id": 214536502,
    "category": "0",
    "click_count": 1523,
    "purchase_count": 145,
    "revenue": 1450.00,
    "rank": 1,
    "click_to_purchase_ratio": 0.0952
  }
]
```

##### `GET /items/categories/{date}`
ì¸ê¸° ì¹´í…Œê³ ë¦¬ Top 20

**Path Parameters**:
- `date` (str): ë‚ ì§œ (YYYY-MM-DD)

**Query Parameters**:
- `limit` (int, optional): ë°˜í™˜í•  ìµœëŒ€ ê°œìˆ˜ (default: 20)

**ì‘ë‹µ ì˜ˆì‹œ**:
```json
[
  {
    "metric_date": "2014-04-07",
    "category": "0",
    "click_count": 50000,
    "purchase_count": 5000,
    "revenue": 50000.00,
    "rank": 1
  }
]
```

##### `GET /items/trending`
íŠ¸ë Œë”© ìƒí’ˆ (ìµœê·¼ 3ì¼ ë¹„êµ)

**Query Parameters**:
- `limit` (int, optional): ë°˜í™˜í•  ìµœëŒ€ ê°œìˆ˜ (default: 10)

**ì‘ë‹µ ì˜ˆì‹œ**:
```json
[
  {
    "item_id": 214536502,
    "category": "0",
    "recent_clicks": 1523,
    "recent_purchases": 145,
    "trend_score": 2.34
  }
]
```

---

## ğŸ“Š ëª¨ë‹ˆí„°ë§ ë° ëŒ€ì‹œë³´ë“œ

### Streamlit Dashboard

**ì ‘ì†**: http://localhost:8501

#### ì£¼ìš” í™”ë©´

1. **í™ˆ (Overview)**
   - ì „ì²´ ì‹œìŠ¤í…œ ìš”ì•½ í†µê³„
   - ìµœê·¼ 24ì‹œê°„ ì´ìƒ íƒì§€ í˜„í™©
   - ì£¼ìš” ì§€í‘œ ì¹´ë“œ (í´ë¦­, êµ¬ë§¤, ì „í™˜ìœ¨)

2. **ì´ìƒ íƒì§€ (Anomalies)**
   - ì‹¤ì‹œê°„ ì´ìƒ ì„¸ì…˜ í…Œì´ë¸”
   - ì´ìƒ ìœ í˜•ë³„ ë¶„í¬ íŒŒì´ ì°¨íŠ¸
   - ì‹œê°„ëŒ€ë³„ ì´ìƒ ë°œìƒ ì¶”ì´ ë¼ì¸ ì°¨íŠ¸
   - ìµœê·¼ 1ì‹œê°„ ì´ìƒ ì„¸ì…˜ ìˆ˜ (ìë™ ê°±ì‹ )

3. **ì¼ë³„ ë©”íŠ¸ë¦­ (Daily Metrics)**
   - ë‚ ì§œë³„ í´ë¦­/êµ¬ë§¤ íŠ¸ë Œë“œ ë¼ì¸ ì°¨íŠ¸
   - ì „í™˜ìœ¨ íŠ¸ë Œë“œ ë¼ì¸ ì°¨íŠ¸
   - í‰ê·  ì„¸ì…˜ ì‹œê°„ ë°” ì°¨íŠ¸
   - ì´ ë§¤ì¶œ ë° í‰ê·  ì£¼ë¬¸ ê¸ˆì•¡ íŠ¸ë Œë“œ

4. **ì „í™˜ í¼ë„ (Funnel)**
   - ì„¸ì…˜ í¼ë„ ë‹¨ê³„ë³„ ì „í™˜ìœ¨ (Funnel Chart)
   - ë‹¨ê³„ë³„ ì´íƒˆë¥  ë¶„ì„
   - ë‚ ì§œë³„ í¼ë„ ë¹„êµ

5. **ì¸ê¸° ìƒí’ˆ (Popular Items)**
   - Top 10 ìƒí’ˆ ë°” ì°¨íŠ¸ (í´ë¦­, êµ¬ë§¤, ë§¤ì¶œ)
   - Top 10 ì¹´í…Œê³ ë¦¬ íŒŒì´ ì°¨íŠ¸
   - í´ë¦­-êµ¬ë§¤ ì „í™˜ìœ¨ ìŠ¤ìºí„° í”Œë¡¯

#### ìë™ ê°±ì‹ 
- **ê°±ì‹  ì£¼ê¸°**: 30ì´ˆ
- **ê°±ì‹  ë°©ë²•**: Streamlit `st.experimental_rerun()`
- **í‘œì‹œ**: ìš°ì¸¡ ìƒë‹¨ì— "Last updated: 2024-01-23 10:15:30" í‘œì‹œ

### Spark UI

#### Spark Master UI
**ì ‘ì†**: http://localhost:8080

**ì •ë³´**:
- Worker ë…¸ë“œ ìƒíƒœ (CPU, ë©”ëª¨ë¦¬)
- Running Applications
- Completed Applications
- í´ëŸ¬ìŠ¤í„° ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ë¥ 

#### Spark Streaming UI
**ì ‘ì†**: http://localhost:4040 (Streaming ì‘ì—… ì‹¤í–‰ ì¤‘ì¼ ë•Œë§Œ)

**ì •ë³´**:
- Streaming íƒ­: Batch Duration, Processing Time, Scheduling Delay
- SQL íƒ­: ì‹¤í–‰ëœ ì¿¼ë¦¬ ê³„íš
- Executors íƒ­: Executor ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ë¥ 
- Environment íƒ­: Spark ì„¤ì •

**ì£¼ìš” ë©”íŠ¸ë¦­**:
- **Input Rate**: ì´ˆë‹¹ ì½ì€ ë ˆì½”ë“œ ìˆ˜
- **Processing Time**: ë°°ì¹˜ ì²˜ë¦¬ ì‹œê°„
- **Total Delay**: ìŠ¤ì¼€ì¤„ë§ ì§€ì—° ì‹œê°„
- **Batch Duration**: 5ì´ˆ (ì„¤ì •ê°’)

### Airflow UI

**ì ‘ì†**: http://localhost:8082

**ë¡œê·¸ì¸**: `admin` / `admin`

**ì •ë³´**:
- DAG ëª©ë¡ ë° ìƒíƒœ
- DAG Run íˆìŠ¤í† ë¦¬
- Task ì‹¤í–‰ ë¡œê·¸
- ë‹¤ìŒ ì‹¤í–‰ ì˜ˆì • ì‹œê°„

**ì£¼ìš” DAG**:
- `daily_batch_pipeline`: ì¼ë³„ ë°°ì¹˜ ë¶„ì„ íŒŒì´í”„ë¼ì¸

**ìˆ˜ë™ íŠ¸ë¦¬ê±°**:
1. DAG ì´ë¦„ í´ë¦­
2. ìš°ì¸¡ ìƒë‹¨ "Trigger DAG" ë²„íŠ¼ í´ë¦­
3. ì‹¤í–‰ ë‚ ì§œ ì„ íƒ (ì„ íƒ ì‚¬í•­)
4. "Trigger" ë²„íŠ¼ í´ë¦­

### MinIO Console

**ì ‘ì†**: http://localhost:9001

**ë¡œê·¸ì¸**: `minioadmin` / `minioadmin`

**ì •ë³´**:
- ë²„í‚· ëª©ë¡ ë° í¬ê¸°
- ê°ì²´ ëª©ë¡ (íŒŒì¼)
- íŒŒí‹°ì…˜ êµ¬ì¡° (dt=YYYY-MM-DD/hour=HH)
- ìŠ¤í† ë¦¬ì§€ ì‚¬ìš©ëŸ‰

**ì£¼ìš” ë²„í‚·**:
- `km-data-lake`: í´ë¦­/êµ¬ë§¤ ì´ë²¤íŠ¸ ì›ë³¸ ë°ì´í„°

### Kafka Monitoring (JMX)

#### JMX í¬íŠ¸
- **kafka-1**: 19092
- **kafka-2**: 19093
- **kafka-3**: 19094

#### ì£¼ìš” ë©”íŠ¸ë¦­

```bash
# JConsole/VisualVMìœ¼ë¡œ ì—°ê²°
# localhost:19092

# ì£¼ìš” ë©”íŠ¸ë¦­ ê²½ë¡œ
kafka.server:type=BrokerTopicMetrics,name=MessagesInPerSec
kafka.server:type=BrokerTopicMetrics,name=BytesInPerSec
kafka.server:type=ReplicaManager,name=UnderReplicatedPartitions
kafka.controller:type=KafkaController,name=ActiveControllerCount
```

#### CLIë¡œ ë©”íŠ¸ë¦­ ì¡°íšŒ

```bash
# Topic ìƒíƒœ
docker exec kafka-1 kafka-topics \
  --bootstrap-server kafka-1:29092 \
  --describe --topic km.clicks.raw.v1

# Consumer Group Lag
docker exec kafka-1 kafka-consumer-groups \
  --bootstrap-server kafka-1:29092 \
  --describe --group anomaly-detector
```

---

## ğŸ›¡ï¸ ìš´ì˜ ë° ì¥ì•  ëŒ€ì‘

### ìš´ì˜ ë¬¸ì„œ ì¸ë±ìŠ¤

ìƒì„¸í•œ ìš´ì˜ ë° ì¥ì•  ëŒ€ì‘ ë¬¸ì„œëŠ” [docs](docs/) í´ë”ì— ìˆìŠµë‹ˆë‹¤.

| ë¬¸ì„œ | ì„¤ëª… | ë§í¬ |
|------|------|------|
| **ìš´ì˜ ë¬¸ì„œ ì¸ë±ìŠ¤** | ëª¨ë“  ìš´ì˜ ë¬¸ì„œì˜ ì‹œì‘ì  | [OPERATIONS_README.md](docs/OPERATIONS_README.md) |
| **Kafka ì„¤ê³„ ë¬¸ì„œ** | Producer, Topic, Error Handling | [KAFKA_DESIGN.md](docs/KAFKA_DESIGN.md) |
| **Spark Streaming ì„¤ê³„** | ì‹¤ì‹œê°„ ì²˜ë¦¬ ë¡œì§ ë° Window | [SPARK_STREAMING_DESIGN.md](docs/SPARK_STREAMING_DESIGN.md) |
| **Airflow DAG ì„¤ê³„** | ë°°ì¹˜ ì‘ì—… ìŠ¤ì¼€ì¤„ë§ ë° ì˜ì¡´ì„± | [AIRFLOW_DAG_DESIGN.md](docs/AIRFLOW_DAG_DESIGN.md) |
| **API ì„œë¹™ ì„¤ê³„** | FastAPI ì—”ë“œí¬ì¸íŠ¸ ë° DB ì¿¼ë¦¬ | [API_SERVING_DESIGN.md](docs/API_SERVING_DESIGN.md) |
| **ëª¨ë‹ˆí„°ë§ ì „ëµ** | ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ë° ì‹œê°í™” | [MONITORING_STRATEGY.md](docs/MONITORING_STRATEGY.md) |
| **ì¥ì•  ë³µêµ¬ ì „ëµ** | ì»´í¬ë„ŒíŠ¸ë³„ ì¥ì•  ì‹œë‚˜ë¦¬ì˜¤ ë° ëŒ€ì‘ | [FAILURE_RECOVERY_STRATEGY.md](docs/FAILURE_RECOVERY_STRATEGY.md) |
| **Fallback/Alert ì „ëµ** | ì•Œë¦¼ ì„¤ì • ë° Fallback ë¡œì§ | [FALLBACK_ALERT_STRATEGY.md](docs/FALLBACK_ALERT_STRATEGY.md) |
| **ë¶€í•˜ í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤** | ë¶€í•˜ í…ŒìŠ¤íŠ¸ ê³„íš ë° ì‹¤í–‰ | [LOAD_TEST_SCENARIO.md](docs/LOAD_TEST_SCENARIO.md) |

### ì£¼ìš” ì¥ì•  ì‹œë‚˜ë¦¬ì˜¤

#### 1. Kafka Broker Down

**ì¦ìƒ**:
- Producerì—ì„œ `NetworkException` ë°œìƒ
- Consumer Rebalancing ë°œìƒ

**ìë™ ëŒ€ì‘**:
- KafkaëŠ” ìë™ìœ¼ë¡œ Leader Election ìˆ˜í–‰
- ProducerëŠ” ë‹¤ë¥¸ Brokerë¡œ ìë™ ì¬ë¼ìš°íŒ…
- ConsumerëŠ” Rebalancing í›„ ê³„ì† ì²˜ë¦¬

**ìˆ˜ë™ ëŒ€ì‘**:
```bash
# Broker ì¬ì‹œì‘
docker start kafka-1

# ISR ë³µêµ¬ í™•ì¸
docker exec kafka-2 kafka-topics \
  --bootstrap-server kafka-2:29093 \
  --describe --topic km.clicks.raw.v1
```

#### 2. Spark Streaming OOM

**ì¦ìƒ**:
- Executor ì¢…ë£Œ (`OutOfMemoryError`)
- Streaming UIì—ì„œ Batch ì‹¤íŒ¨

**ì›ì¸**:
- ëŒ€ìš©ëŸ‰ ìœˆë„ìš° ì§‘ê³„
- GC ì˜¤ë²„í—¤ë“œ

**ëŒ€ì‘**:
```bash
# Executor ë©”ëª¨ë¦¬ ì¦ê°€
spark-submit --executor-memory 4g ...

# ìœˆë„ìš° í¬ê¸° ì¶•ì†Œ ë˜ëŠ” íŒŒí‹°ì…˜ ì¦ê°€
# anomaly_detector.py ìˆ˜ì •
```

#### 3. PostgreSQL Connection Pool Exhaustion

**ì¦ìƒ**:
- APIì—ì„œ `TimeoutError` ë°œìƒ
- Spark ì‘ì—… ì‹¤íŒ¨

**ì›ì¸**:
- ë™ì‹œ ì—°ê²° ìˆ˜ ì´ˆê³¼

**ëŒ€ì‘**:
```bash
# PostgreSQL max_connections ì¦ê°€
docker exec -it postgres psql -U admin -d clickstream
ALTER SYSTEM SET max_connections = 200;
SELECT pg_reload_conf();

# Connection Pool ì„¤ì • ì¡°ì • (api/config.py)
SQLALCHEMY_POOL_SIZE = 20
SQLALCHEMY_MAX_OVERFLOW = 10
```

#### 4. S3 Sink Connector ì‹¤íŒ¨

**ì¦ìƒ**:
- Kafka Connectì—ì„œ Connector FAILED ìƒíƒœ
- S3ì— ìƒˆ íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•ŠìŒ

**ì›ì¸**:
- MinIO ì—°ê²° ì‹¤íŒ¨
- S3 ê¶Œí•œ ë¬¸ì œ

**ëŒ€ì‘**:
```bash
# Connector ìƒíƒœ í™•ì¸
curl http://localhost:8083/connectors/s3-sink-clicks/status

# Connector ì¬ì‹œì‘
curl -X POST http://localhost:8083/connectors/s3-sink-clicks/restart

# MinIO ìƒíƒœ í™•ì¸
docker logs minio
```

### íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”

```bash
# ì „ì²´ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” (ë°ì´í„° ì‚­ì œ)
bash scripts/reset-pipeline.sh
```

**ìˆ˜í–‰ ì‘ì—…**:
1. ëª¨ë“  ì»¨í…Œì´ë„ˆ ì¤‘ì§€ ë° ì‚­ì œ
2. Docker ë³¼ë¥¨ ì‚­ì œ (Kafka, PostgreSQL, MinIO ë°ì´í„°)
3. ì¬ì‹œì‘
4. Topic ì¬ìƒì„±
5. ìŠ¤í‚¤ë§ˆ ì¬ë“±ë¡
6. Connector ì¬ì„¤ì •

---

## ğŸ§ª í…ŒìŠ¤íŠ¸

### 1. í—¬ìŠ¤ ì²´í¬

```bash
# API í—¬ìŠ¤ ì²´í¬
curl http://localhost:8000/health

# PostgreSQL ì—°ê²° í™•ì¸
docker exec -it postgres psql -U admin -d clickstream -c "SELECT 1;"

# Kafka í† í”½ í™•ì¸
docker exec kafka-1 kafka-topics --list --bootstrap-server kafka-1:29092

# MinIO í™•ì¸
aws s3 ls --endpoint-url http://localhost:9000
```

### 2. ë°ì´í„° í™•ì¸

```bash
# PostgreSQL ë°ì´í„° í™•ì¸
docker exec -it postgres psql -U admin -d clickstream -c "
SELECT 
  (SELECT COUNT(*) FROM anomaly_sessions) as anomalies,
  (SELECT COUNT(*) FROM daily_metrics) as daily_metrics,
  (SELECT COUNT(*) FROM popular_items) as popular_items,
  (SELECT COUNT(*) FROM session_funnel) as session_funnel;
"

# Kafka Consumer Group Lag
docker exec kafka-1 kafka-consumer-groups \
  --bootstrap-server kafka-1:29092 \
  --describe --group anomaly-detector

# S3 íŒŒì¼ ê°œìˆ˜
aws s3 ls s3://km-data-lake/topics/ \
  --endpoint-url http://localhost:9000 --recursive | wc -l
```

### 3. ë¶€í•˜ í…ŒìŠ¤íŠ¸ (ì„ íƒ)

#### Locustë¥¼ ì´ìš©í•œ API ë¶€í•˜ í…ŒìŠ¤íŠ¸

```bash
# Locust ì„¤ì¹˜
pip install locust

# ë¶€í•˜ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
locust -f scripts/load_test.py --host http://localhost:8000
```

**ì›¹ UI**: http://localhost:8089

**ì‹œë‚˜ë¦¬ì˜¤**:
- ì´ìƒ ì„¸ì…˜ ì¡°íšŒ
- ì¼ë³„ ë©”íŠ¸ë¦­ ì¡°íšŒ
- ì¸ê¸° ìƒí’ˆ ì¡°íšŒ
- í˜¼í•© ì‹œë‚˜ë¦¬ì˜¤

#### Producer ë¶€í•˜ í…ŒìŠ¤íŠ¸

```bash
# 100ë°°ì†ìœ¼ë¡œ ì „ì†¡
python producer_clicks.py --replay-speed 100.0

# 60ì´ˆë§ˆë‹¤ ì´ìƒ íŒ¨í„´ ì£¼ì…
python producer_clicks.py --anomaly-interval 60
```

### 4. ì¥ì•  ì‹œë®¬ë ˆì´ì…˜

```bash
# Kafka Broker Down
docker stop kafka-1

# ëŒ€ê¸° (5ì´ˆ)
sleep 5

# ì¬ì‹œì‘
docker start kafka-1

# Spark Worker Down
docker stop spark-worker-1

# PostgreSQL Down
docker stop postgres
```

---

## ğŸ›‘ ì¤‘ì§€ ë° ì •ë¦¬

### ì „ì²´ ì„œë¹„ìŠ¤ ì¤‘ì§€

```bash
cd docker
docker-compose down
```

### ë°ì´í„°ê¹Œì§€ ì‚­ì œ (ë³¼ë¥¨ ì‚­ì œ)

```bash
cd docker
docker-compose down -v
```

**ì‚­ì œë˜ëŠ” ë°ì´í„°**:
- Kafka í† í”½ ë°ì´í„°
- PostgreSQL í…Œì´ë¸” ë°ì´í„°
- MinIO S3 ê°ì²´
- Zookeeper ë©”íƒ€ë°ì´í„°

### íŠ¹ì • ì„œë¹„ìŠ¤ë§Œ ì¬ì‹œì‘

```bash
# Spark Streaming ì¬ì‹œì‘
docker-compose restart spark-master spark-worker-1 spark-worker-2

# Kafka Cluster ì¬ì‹œì‘
docker-compose restart kafka-1 kafka-2 kafka-3

# API ì¬ì‹œì‘
docker-compose restart api

# Dashboard ì¬ì‹œì‘
docker-compose restart dashboard
```

---

## ğŸš€ í™•ì¥ ë° ê°œì„  ë°©í–¥

### ë‹¨ê¸° ê°œì„ 

1. **Prometheus + Grafana í†µí•©**
   - JMX Exporterë¡œ Kafka/Spark ë©”íŠ¸ë¦­ ìˆ˜ì§‘
   - Grafana ëŒ€ì‹œë³´ë“œë¡œ ì‹œê°í™”
   - ì•Œë¦¼ ê·œì¹™ ì„¤ì •

2. **Authentication/Authorization**
   - APIì— JWT ì¸ì¦ ì¶”ê°€
   - ëŒ€ì‹œë³´ë“œì— ì‚¬ìš©ì ë¡œê·¸ì¸ ì¶”ê°€

3. **CI/CD íŒŒì´í”„ë¼ì¸**
   - GitHub Actionsë¡œ ìë™ í…ŒìŠ¤íŠ¸
   - Docker ì´ë¯¸ì§€ ìë™ ë¹Œë“œ ë° í‘¸ì‹œ

4. **ë¡œê¹… ì¤‘ì•™í™”**
   - ELK Stack (Elasticsearch, Logstash, Kibana)
   - Fluentdë¡œ ë¡œê·¸ ìˆ˜ì§‘

### ì¤‘ê¸° ê°œì„ 

1. **Kubernetes ë§ˆì´ê·¸ë ˆì´ì…˜**
   - Docker Compose â†’ Kubernetes
   - Helm Chart ì‘ì„±
   - Auto-scaling ì„¤ì •

2. **ì‹¤ì‹œê°„ ì•Œë¦¼**
   - Slack/Email ì•Œë¦¼ ì—°ë™
   - ì´ìƒ íƒì§€ ì‹œ ì¦‰ì‹œ ì•Œë¦¼
   - PagerDuty í†µí•©

3. **ML ê¸°ë°˜ ì´ìƒ íƒì§€**
   - Isolation Forest, Autoencoder
   - ì˜¨ë¼ì¸ í•™ìŠµ íŒŒì´í”„ë¼ì¸
   - Feature Engineering

4. **A/B í…ŒìŠ¤íŠ¸ í”Œë«í¼**
   - ì‹¤í—˜êµ°/ëŒ€ì¡°êµ° ë¶„ë¦¬
   - ì „í™˜ìœ¨ ë¹„êµ ë¶„ì„

### ì¥ê¸° ê°œì„ 

1. **ë©€í‹° ë¦¬ì „ ë°°í¬**
   - Active-Active ì•„í‚¤í…ì²˜
   - Cross-region Replication
   - ê¸€ë¡œë²Œ ë¡œë“œ ë°¸ëŸ°ì‹±

2. **Data Mesh ì•„í‚¤í…ì²˜**
   - ë„ë©”ì¸ë³„ ë°ì´í„° ì†Œìœ ê¶Œ
   - Self-serve ë°ì´í„° í”Œë«í¼
   - Federated Governance

3. **ì‹¤ì‹œê°„ ì¶”ì²œ ì‹œìŠ¤í…œ**
   - Collaborative Filtering
   - ì‹¤ì‹œê°„ ê°œì¸í™”
   - A/B í…ŒìŠ¤íŠ¸ í†µí•©

---

## ğŸ“ ë¼ì´ì„¼ìŠ¤

MIT License

Copyright (c) 2026 Clickstream Guardian

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

---

## ğŸ™ ê°ì‚¬ì˜ ë§

- **YOOCHOOSE**: ë°ì´í„°ì…‹ ì œê³µ
- **Apache Software Foundation**: Kafka, Spark, Airflow
- **Confluent**: Kafka ì—ì½”ì‹œìŠ¤í…œ
- **FastAPI**: í˜„ëŒ€ì ì¸ Python ì›¹ í”„ë ˆì„ì›Œí¬
- **Streamlit**: ë¹ ë¥¸ ëŒ€ì‹œë³´ë“œ ê°œë°œ

---

## ğŸ“§ ë¬¸ì˜ ë° ê¸°ì—¬

### ì´ìŠˆ ì œë³´

ë²„ê·¸ ë°œê²¬ì´ë‚˜ ê¸°ëŠ¥ ì œì•ˆì€ [GitHub Issues](https://github.com/yourusername/Clickstream-Guardian/issues)ì— ë“±ë¡í•´ì£¼ì„¸ìš”.

### Pull Request

ê¸°ì—¬ëŠ” ì–¸ì œë‚˜ í™˜ì˜í•©ë‹ˆë‹¤!

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

### ì½”ë”© ì»¨ë²¤ì…˜

- **Python**: PEP 8
- **Commit Message**: Conventional Commits
- **Branch**: `feature/`, `bugfix/`, `hotfix/`

---

<div align="center">

**Built with â¤ï¸ using**

![Kafka](https://img.shields.io/badge/Apache%20Kafka-231F20?style=for-the-badge&logo=apache-kafka&logoColor=white)
![Spark](https://img.shields.io/badge/Apache%20Spark-E25A1C?style=for-the-badge&logo=apache-spark&logoColor=white)
![Airflow](https://img.shields.io/badge/Apache%20Airflow-017CEE?style=for-the-badge&logo=apache-airflow&logoColor=white)
![FastAPI](https://img.shields.io/badge/FastAPI-009688?style=for-the-badge&logo=fastapi&logoColor=white)
![Streamlit](https://img.shields.io/badge/Streamlit-FF4B4B?style=for-the-badge&logo=streamlit&logoColor=white)
![PostgreSQL](https://img.shields.io/badge/PostgreSQL-4169E1?style=for-the-badge&logo=postgresql&logoColor=white)
![Docker](https://img.shields.io/badge/Docker-2496ED?style=for-the-badge&logo=docker&logoColor=white)
![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)

â­ **ì´ í”„ë¡œì íŠ¸ê°€ ë„ì›€ì´ ë˜ì—ˆë‹¤ë©´ Starë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”!** â­

</div>
